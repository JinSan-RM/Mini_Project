{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "Setup complete ✅ (12 CPUs, 15.6 GB RAM, 50.3/251.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train model=yolov8n.pt data=coco128.yaml epochs=3 imgsz=640\n",
    "# !yolo detect train data=D:/WEVEN/htdocs/tfjs/ultralytics/datasets/step1/data.yaml model=D:/WEVEN/htdocs/tfjs/ultralytics/yolov8n.pt epochs=10 imgsz=640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "modelName = 'step2_logotrace'\n",
    "# Load the model.\n",
    "ROOT_PATH = os.getcwd() + \"\\\\\";\n",
    "# model = YOLO(ROOT_PATH + 'ultralytics/yolov8n.pt')\n",
    "model = YOLO('/code/ultralytics/yolov8m.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ultralytics.yolo.engine.model.YOLO object at 0x7f215e788d90>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.120 🚀 Python-3.8.10 torch-2.0.1+cu117 CPU\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=/code/ultralytics/yolov8n.pt, data=/tf/datasets/step1/data.yaml, epochs=2, patience=50, batch=8, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step1_section, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step1_section4\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3013383 parameters, 3013367 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/step1_section4', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /tf/datasets/step1/train/labels.cache... 1820 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1820/1820 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/datasets/step1/valid/labels.cache... 392 images, 0 backgrounds, 0 corrupt: 100%|██████████| 392/392 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/step1_section4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step1_section4\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/2         0G      1.736      3.731      1.474         92        320: 100%|██████████| 228/228 [11:50<00:00,  3.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [01:17<00:00,  3.10s/it]\n",
      "                   all        392       3657      0.499      0.233      0.197      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/2         0G      1.263      2.415      1.136         65        320: 100%|██████████| 228/228 [12:05<00:00,  3.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [01:46<00:00,  4.28s/it]\n",
      "                   all        392       3657      0.477       0.34      0.258       0.18\n",
      "\n",
      "2 epochs completed in 0.454 hours.\n",
      "Optimizer stripped from runs/detect/step1_section4/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/step1_section4/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/step1_section4/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.120 🚀 Python-3.8.10 torch-2.0.1+cu117 CPU\n",
      "Model summary (fused): 168 layers, 3008183 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:51<00:00,  2.08s/it]\n",
      "                   all        392       3657      0.477       0.34      0.258       0.18\n",
      "                banner        392        728      0.543      0.739      0.637      0.504\n",
      "                2block        392        276      0.188      0.438      0.159       0.12\n",
      "                3block        392        513      0.236      0.499      0.223      0.162\n",
      "                4block        392        580       0.32      0.636      0.373      0.282\n",
      "                5block        392        287        0.2      0.125      0.149        0.1\n",
      "                6block        392         97      0.143      0.227      0.111     0.0732\n",
      "                7block        392         19          1          0    0.00456    0.00339\n",
      "                8block        392          5          1          0     0.0019   0.000859\n",
      "                nblock        392         12          1          0   0.000586    0.00022\n",
      "                  typo        392        167      0.122      0.024     0.0792     0.0448\n",
      "               gallery        392        223      0.162      0.238      0.143      0.105\n",
      "                header        392        382      0.569      0.649      0.646      0.342\n",
      "                footer        392        368      0.714       0.84      0.821      0.603\n",
      "Speed: 0.3ms preprocess, 31.1ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step1_section4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "modelName = 'step1_section'\n",
    "# Load the model.\n",
    "ROOT_PATH = os.getcwd() + \"\\\\\";\n",
    "# model = YOLO(ROOT_PATH + 'ultralytics/yolov8n.pt')\n",
    "model = YOLO('/code/ultralytics/yolov8m.pt')\n",
    " \n",
    "# Training.\n",
    "results = model.train(\n",
    "   # data=ROOT_PATH + 'ultralytics/datasets/step1/data.yaml',\n",
    "   #data='/tf/datasets/step1/data.yaml',\n",
    "   data='/tf/datasets/logotest/data.yaml',\n",
    "   imgsz=640, # origin 640\n",
    "   epochs=300, # origin 120\n",
    "   batch=2,\n",
    "   name=modelName\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export\n",
    "\n",
    "Export a YOLOv8 model to any supported format with the `format` argument, i.e. `format=onnx`.\n",
    "\n",
    "- 💡 ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.  \n",
    "- 💡 ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n",
    "\n",
    "\n",
    "| Format                                                                     | `format=`          | Model                     |\n",
    "|----------------------------------------------------------------------------|--------------------|---------------------------|\n",
    "| [PyTorch](https://pytorch.org/)                                            | -                  | `yolov8n.pt`              |\n",
    "| [TorchScript](https://pytorch.org/docs/stable/jit.html)                    | `torchscript`      | `yolov8n.torchscript`     |\n",
    "| [ONNX](https://onnx.ai/)                                                   | `onnx`             | `yolov8n.onnx`            |\n",
    "| [OpenVINO](https://docs.openvino.ai/latest/index.html)                     | `openvino`         | `yolov8n_openvino_model/` |\n",
    "| [TensorRT](https://developer.nvidia.com/tensorrt)                          | `engine`           | `yolov8n.engine`          |\n",
    "| [CoreML](https://github.com/apple/coremltools)                             | `coreml`           | `yolov8n.mlmodel`         |\n",
    "| [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`      | `yolov8n_saved_model/`    |\n",
    "| [TensorFlow GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`               | `yolov8n.pb`              |\n",
    "| [TensorFlow Lite](https://www.tensorflow.org/lite)                         | `tflite`           | `yolov8n.tflite`          |\n",
    "| [TensorFlow Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`          | `yolov8n_edgetpu.tflite`  |\n",
    "| [TensorFlow.js](https://www.tensorflow.org/js)                             | `tfjs`             | `yolov8n_web_model/`      |\n",
    "| [PaddlePaddle](https://github.com/PaddlePaddle)                            | `paddle`           | `yolov8n_paddle_model/`   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /tf/notebook/step1/nike.jpeg: 320x64 2 banners, 2 3blocks, 1 footer, 101.2ms\n",
      "Speed: 8.6ms preprocess, 101.2ms inference, 13.3ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'banner', 1: '2block', 2: '3block', 3: '4block', 4: '5block', 5: '6block', 6: '7block', 7: '8block', 8: 'nblock', 9: 'typo', 10: 'gallery', 11: 'header', 12: 'footer'}\n",
       " orig_img: array([[[246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         ...,\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246]],\n",
       " \n",
       "        [[246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         ...,\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246]],\n",
       " \n",
       "        [[246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         ...,\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246],\n",
       "         [246, 246, 246]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (4096, 726)\n",
       " path: '/tf/notebook/step1/nike.jpeg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 8.620977401733398, 'inference': 101.21726989746094, 'postprocess': 13.284921646118164}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "ROOT_PATH = os.getcwd() + \"\\\\\";\n",
    "modelName = \"step1_section\"\n",
    "model = YOLO( '/tf/notebook/step1/runs/detect/'+modelName+'/weights/best.pt')  # load a pretrained YOLOv8n detection model\n",
    "# model.train(data='coco128.yaml', epochs=3)  # train the model\n",
    "model( '/tf/notebook/step1/nike.jpeg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 검증 실행 ( 파이썬 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.120 🚀 Python-3.8.10 torch-2.0.1+cu117 CPU\n",
      "Model summary (fused): 168 layers, 3008183 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/datasets/step1/valid/labels.cache... 392 images, 0 backgrounds\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        392       3657      0.477       0.34      0.258       0.18\n",
      "                banner        392        728      0.543      0.739      0.637      0.504\n",
      "                2block        392        276      0.188      0.438      0.159       0.12\n",
      "                3block        392        513      0.236      0.499      0.223      0.162\n",
      "                4block        392        580       0.32      0.636      0.373      0.282\n",
      "                5block        392        287        0.2      0.125      0.149        0.1\n",
      "                6block        392         97      0.143      0.227      0.111     0.0732\n",
      "                7block        392         19          1          0    0.00456    0.00339\n",
      "                8block        392          5          1          0     0.0019   0.000859\n",
      "                nblock        392         12          1          0   0.000586    0.00022\n",
      "                  typo        392        167      0.122      0.024     0.0792     0.0448\n",
      "               gallery        392        223      0.162      0.238      0.143      0.105\n",
      "                header        392        382      0.569      0.649      0.646      0.342\n",
      "                footer        392        368      0.714       0.84      0.821      0.603\n",
      "Speed: 0.1ms preprocess, 9.1ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=val model=/tf/notebook/step1/runs/detect/step1_section/weights/best.pt data=/tf/datasets/step1/data.yaml\n",
    "\n",
    "# !python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.120 🚀 Python-3.8.10 torch-2.0.1+cu117 CPU\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/datasets/step1/valid/labels.cache... 392 images, 0 backgrounds, 0 corrupt: 100%|██████████| 392/392 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:34<00:00,  1.40s/it]\n",
      "                   all        392       3657      0.477       0.34      0.258       0.18\n",
      "                banner        392        728      0.543      0.739      0.637      0.504\n",
      "                2block        392        276      0.188      0.438      0.159       0.12\n",
      "                3block        392        513      0.236      0.499      0.223      0.162\n",
      "                4block        392        580       0.32      0.636      0.373      0.282\n",
      "                5block        392        287        0.2      0.125      0.149        0.1\n",
      "                6block        392         97      0.143      0.227      0.111     0.0732\n",
      "                7block        392         19          1          0    0.00456    0.00339\n",
      "                8block        392          5          1          0     0.0019   0.000859\n",
      "                nblock        392         12          1          0   0.000586    0.00022\n",
      "                  typo        392        167      0.122      0.024     0.0792     0.0448\n",
      "               gallery        392        223      0.162      0.238      0.143      0.105\n",
      "                header        392        382      0.569      0.649      0.646      0.342\n",
      "                footer        392        368      0.714       0.84      0.821      0.603\n",
      "Speed: 0.1ms preprocess, 12.4ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x7f39388d08b0>\n",
       "fitness: 0.18786970660497354\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([     0.5036,     0.11953,     0.16243,     0.28213,     0.10024,    0.073216,   0.0033929,  0.00085931,  0.00022049,    0.044833,     0.10538,     0.34248,     0.60331])\n",
       "names: {0: 'banner', 1: '2block', 2: '3block', 3: '4block', 4: '5block', 5: '6block', 6: '7block', 7: '8block', 8: 'nblock', 9: 'typo', 10: 'gallery', 11: 'header', 12: 'footer'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.4766893023428906, 'metrics/recall(B)': 0.33964596288834586, 'metrics/mAP50(B)': 0.25757332514889897, 'metrics/mAP50-95(B)': 0.18012486010009293, 'fitness': 0.18786970660497354}\n",
       "save_dir: PosixPath('runs/detect/val2')\n",
       "speed: {'preprocess': 0.12867061459288304, 'inference': 12.355379304107355, 'loss': 0.00011860107888980788, 'postprocess': 1.8003674186005885}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tf/notebook/step1/runs/detect/step1_section/weights/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m imgPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tf/notebook/step1/nike.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(imgPath, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tf/notebook/step1/runs/detect/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodelName\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model2\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39mimgPath, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 이미지 RGB 변환\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/model.py:107\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/model.py:156\u001b[0m, in \u001b[0;36mYOLO._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    154\u001b[0m suffix \u001b[38;5;241m=\u001b[39m Path(weights)\u001b[38;5;241m.\u001b[39msuffix\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/nn/tasks.py:578\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    577\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/nn/tasks.py:518\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    516\u001b[0m file \u001b[38;5;241m=\u001b[39m attempt_download_asset(weight)  \u001b[38;5;66;03m# search online if missing locally\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, file  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tf/notebook/step1/runs/detect/step1_section/weights/best.pt'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "ROOT_PATH = os.getcwd() + \"\\\\\";\n",
    "\n",
    "modelName = 'step1_section'\n",
    "# from google.colab.patches import cv2_imshow\n",
    "imgPath = \"/tf/notebook/step1/nike.jpeg\"\n",
    "img = cv2.imread(imgPath, 0)\n",
    "model2 = YOLO( \"/tf/notebook/step1/runs/detect/\"+modelName+\"/weights/best.pt\")\n",
    "model2.predict(source=imgPath, show=True, save=True)\n",
    "\n",
    "\n",
    "# 이미지 RGB 변환\n",
    "image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 이미지 표시\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "# plot_bboxes(\"/tf/notebook/step1/nike.jpeg\", pred[0].boxes.boxes, score=False)\n",
    "\n",
    "# print( pred )\n",
    "# success = model2.export(format='saved_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!yolo task=detect mode=predict model=/tf/notebook/step1/runs/detect/step1_section/weights/best.pt source=D:/WEVEN/htdocs/tfjs/ultralytics/sample/nike.jpeg show=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8394f1d6003c2dcf93d83a4b3414963c0c1a824e31306410f3e4bb3f58793a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
