{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fb6a8a",
   "metadata": {},
   "source": [
    "# Ultralytics YOLO format\n",
    "\n",
    "## 1. Directory\n",
    "\n",
    "```powershell\n",
    "<Custom Dataset Directory>\n",
    "****├── 0.jpg\n",
    "├── 0.txt\n",
    "└── ...\n",
    "```\n",
    "\n",
    "### Split Dataset Directory\n",
    "\n",
    "```powershell\n",
    "rico_yolo\n",
    "├── test\n",
    "│   ├── 0.jpg\n",
    "│   ├── 0.txt\n",
    "│   └── ...\n",
    "├── train\n",
    "│   ├── 2.jpg\n",
    "│   ├── 2.txt\n",
    "│   └── ...\n",
    "└── val\n",
    "    ├── 36.jpg\n",
    "    ├── 36.txt\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "## 2. Annotation\n",
    "\n",
    "```\n",
    "<object-class> <x> <y> <width> <height>\n",
    "```\n",
    "\n",
    "### 예시) 2.txt\n",
    "\n",
    "13 0.5 0.4833984375 0.7944444444444444 0.626171875\n",
    "3 0.5 0.758203125 0.7944444444444444 0.0765625\n",
    "21 0.7875 0.758203125 0.16111111111111112 0.065625\n",
    "6 0.5 0.4451171875 0.7944444444444444 0.549609375\n",
    "21 0.5 0.198046875 0.6777777777777778 0.05546875\n",
    "21 0.35381944444444446 0.2568359375 0.3854166666666667 0.062109375\n",
    "8 0.2013888888888889 0.358984375 0.11666666666666667 0.065625\n",
    "8 0.7986111111111112 0.358984375 0.11666666666666667 0.065625\n",
    "\n",
    "## Dataset File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30baae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_text = \"\"\"\n",
    "path: /weven/datasets/\n",
    "train: /weven/datasets/Object/train\n",
    "val: /weven/datasets/rico_yolo/val\n",
    "test: /weven/datasets/rico_yolo/test\n",
    "\n",
    "names:\n",
    "    0: 'typo_text',\n",
    "    1: 'typo_price_num',\n",
    "    2: 'typo_price_dollar',\n",
    "    3: 'typo_price_W',\n",
    "    4: 'typo_price_won_en',\n",
    "    5: 'typo_price_won_kr',\n",
    "    6: 'image_photo',\n",
    "    7: 'image_colorBG',\n",
    "    8: 'image_removeBG',\n",
    "    9: 'icon_arrow_left',\n",
    "    10: 'icon_arrow_top',\n",
    "    11: 'icon_arrow_bottom',\n",
    "    12: 'icon_arrow_right',\n",
    "    13: 'icon_video_play',\n",
    "    14: 'icon_SNS_insta',\n",
    "    15: 'icon_SNS_youtube',\n",
    "    16: 'btn_radius',\n",
    "    17: 'btn_ellipse',\n",
    "    18: 'btn_square'\"\"\"\n",
    "\n",
    "with open('/tf/notebook/datasets/data.yaml', 'w') as file:\n",
    "    file.write(yaml_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8095c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 작업 디렉토리\n",
    "\n",
    "'''powershell\n",
    "weven\n",
    "├── datasets\n",
    "│   └── Obejectblock\n",
    "└── notebooks\n",
    "    └── read_rico_dataset.ipynb\n",
    "\n",
    "'''\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae18f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Training - Preparing  Dataset for YOLO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import functools\n",
    "import itertools\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a94a1",
   "metadata": {},
   "source": [
    "## For parallel computing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "num_workers = 32  # The number of threads or processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6f8b7",
   "metadata": {},
   "source": [
    "# mt_executor = concurrent.futures.ThreadPoolExecutor(num_workers)  \n",
    "# Use ThreadPoolExecutor for CPU-bound tasks.\n",
    "# mp_executor = concurrent.futures.ProcessPoolExecutor(num_workers)  \n",
    "# Use ProcessPoolExecutor for I/O-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3547d",
   "metadata": {},
   "source": [
    "#  dataset path settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshots_path = Path('../datasets/Object/~')  # 66261 '.jpg' and '.json'\n",
    "annotations_path = Path('../datasets/Object/~/')  # 66261 '.png' and 'json'\n",
    "\n",
    "image_ids = pd.Series([int(p.stem) for p in screenshots_path.iterdir() if p.suffix == '.json'], name='image_id')\n",
    "image_ids = image_ids.sort_values().reset_index(drop='index')\n",
    "\n",
    "screenshot_paths = [screenshots_path / f'{image_id}.jpg' for image_id in image_ids]\n",
    "annotation_paths = [annotations_path / f'{image_id}.png' for image_id in image_ids]\n",
    "view_hierarchy_json_paths = [screenshots_path / f'{image_id}.json' for image_id in image_ids]\n",
    "annotation_hierarchy_json_paths = [annotations_path / f'{image_id}.json' for image_id in image_ids]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965b090",
   "metadata": {},
   "source": [
    "## Read hierarchy json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92986973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hierarchy_from_json(json_path):\n",
    "    with open(json_path) as fp:\n",
    "        hierarchy = json.load(fp)\n",
    "        return hierarchy\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    annotation_hierarchies = mp_executor.map(get_hierarchy_from_json, annotation_hierarchy_json_paths)\n",
    "    annotation_hierarchies = list(annotation_hierarchies)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    view_hierarchies = mp_executor.map(get_hierarchy_from_json, view_hierarchy_json_paths)\n",
    "    view_hierarchies = list(view_hierarchies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c9f0e",
   "metadata": {},
   "source": [
    "# Get Image sizes (가로, 세로)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imsize(path):\n",
    "    im = Image.open(path)\n",
    "    return im.size\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    screenshot_sizes = mp_executor.map(get_imsize, screenshot_paths)\n",
    "    screenshot_sizes = list(screenshot_sizes)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    annotation_sizes = mp_executor.map(get_imsize, annotation_paths)\n",
    "    annotation_sizes = list(annotation_sizes)\n",
    "\n",
    "screenshot_sizes_df = pd.DataFrame(screenshot_sizes, columns=['width', 'height'])\n",
    "annotation_sizes_df = pd.DataFrame(annotation_sizes, columns=['width', 'height'])\n",
    "\n",
    "screenshot_sizes_df.insert(0, 'image_id', image_ids)\n",
    "annotation_sizes_df.insert(0, 'image_id', image_ids)\n",
    "screenshot_sizes_df['ratio'] = screenshot_sizes_df['width'] / screenshot_sizes_df['height']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba561a",
   "metadata": {},
   "source": [
    "# Get useful information from hierarchies\n",
    "\n",
    "## A. From view_hierarchies\n",
    "\n",
    "### 1. root size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_size(hierarchy):\n",
    "    return hierarchy['activity']['root']['rel-bounds']\n",
    "\n",
    "root_sizes = map(get_root_size, view_hierarchies)\n",
    "\n",
    "root_sizes_df = pd.DataFrame(root_sizes, columns=['x1', 'y1', 'x2', 'y2'])\n",
    "root_sizes_df.insert(0, 'image_id', image_ids)\n",
    "root_sizes_df['ratio'] = root_sizes_df['x2'] / root_sizes_df['y2']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cb7ef",
   "metadata": {},
   "source": [
    "### 2. root classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_classes(hierarchy):\n",
    "    root_class_name = hierarchy['activity']['root']['class']\n",
    "    return root_class_name\n",
    "\n",
    "root_class_names = list(map(get_root_classes, view_hierarchies))\n",
    "pd.Series(root_class_names).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a546a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## B. From annotaion_hierarchies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_attributes(hierarchy, attribute_name):\n",
    "    uniques = set()\n",
    "\n",
    "    recursive_function = functools.partial(get_unique_attributes, attribute_name=attribute_name)\n",
    "    if type(hierarchy) is list:\n",
    "        uniques_list = map(recursive_function, hierarchy)\n",
    "        uniques |= functools.reduce(lambda x,y: x | y, uniques_list, set())\n",
    "    \n",
    "    if type(hierarchy) is dict:\n",
    "        if attribute_name in hierarchy:\n",
    "            uniques.add(hierarchy[attribute_name])\n",
    "    \n",
    "        if 'children' in hierarchy:\n",
    "            uniques |= recursive_function(hierarchy['children'])\n",
    "    \n",
    "    return uniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c9f07",
   "metadata": {},
   "source": [
    "### 1. componentLabel, 2. iconClass, 3.textButtonClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_component_labels = get_unique_attributes(annotation_hierarchies, 'componentLabel')\n",
    "icon_classes = get_unique_attributes(annotation_hierarchies, 'iconClass')\n",
    "text_button_classes = get_unique_attributes(annotation_hierarchies, 'textButtonClass')\n",
    "\n",
    "all_component_labels = sorted(list(all_component_labels))\n",
    "icon_classes = sorted(list(icon_classes))\n",
    "text_button_classes = sorted(list(text_button_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fd2a3",
   "metadata": {},
   "source": [
    "# Get bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4dad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_and_bounds(hierarchy):\n",
    "    bboxes = []\n",
    "\n",
    "    if 'componentLabel' in hierarchy:\n",
    "        label = hierarchy['componentLabel']\n",
    "        x1, y1, x2, y2 = hierarchy['bounds']\n",
    "        bboxes.append(dict(label=label, x1=x1, y1=y1, x2=x2, y2=y2))\n",
    "\n",
    "    if 'children' in hierarchy:\n",
    "        bboxes += list(itertools.chain.from_iterable(map(get_component_and_bounds, hierarchy['children'])))\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "all_bboxes = list(map(get_component_and_bounds, annotation_hierarchies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b42ff4",
   "metadata": {},
   "source": [
    "# Preprocess dataset\n",
    "\n",
    "### Drop duplicated boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf55a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_bbox_dfs = list(map(lambda x: pd.DataFrame(x).drop_duplicates().reset_index(drop=True), all_bboxes))\n",
    "\n",
    "\n",
    "\n",
    "all_bbox_df = pd.concat(all_bbox_dfs, keys=image_ids, names=['image_id', 'bbox_id'])\n",
    "all_bbox_df = all_bbox_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd7605",
   "metadata": {},
   "source": [
    "### **Find landscape view (wrong matches)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f19889",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape_screenshot_df = screenshot_sizes_df[screenshot_sizes_df['ratio'] > 1]\n",
    "landscaped = all_bbox_df.index.isin(landscape_screenshot_df['image_id'], level='image_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e5b34",
   "metadata": {},
   "source": [
    "### **Find overflowed boxes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotation_sizes = {'width': 1440, 'height': 2560}\n",
    "overflowed = all_bbox_df['x1'] < 0\n",
    "overflowed |= all_bbox_df['y1'] < 0\n",
    "overflowed |= all_bbox_df['x2'] > annotation_sizes['width']\n",
    "overflowed |= all_bbox_df['y2'] > annotation_sizes['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac387a",
   "metadata": {},
   "source": [
    "### **Find boxes with 0 or negative values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c381804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negated = all_bbox_df['x1'] >= all_bbox_df['x2']\n",
    "negated |= all_bbox_df['y1'] >= all_bbox_df['y2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a672cdf",
   "metadata": {},
   "source": [
    "### **Filter out all corrupted boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b22690",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df = all_bbox_df[~(landscaped | overflowed | negated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa41488",
   "metadata": {},
   "source": [
    "# **Convert to YOLO format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = pd.Series(list(all_component_labels))\n",
    "class_index = pd.Series(class_index.index.values, index=class_index)\n",
    "\n",
    "\n",
    "\n",
    "yolo_bbox_df = pd.DataFrame({\n",
    "    'label': pd.Series(class_index[bbox_df['label']].values, index=bbox_df.index),\n",
    "    'x': (bbox_df['x1'] + bbox_df['x2']) / 2 / annotation_sizes['width'],\n",
    "    'y': (bbox_df['y1'] + bbox_df['y2']) / 2 / annotation_sizes['height'],\n",
    "    'width': (bbox_df['x2'] - bbox_df['x1']) / annotation_sizes['width'],\n",
    "    'height': (bbox_df['y2'] - bbox_df['y1']) / annotation_sizes['height'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc7c2d",
   "metadata": {},
   "source": [
    "# Create YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22130d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_yolo_path = Path('tf/datasets/datasets')\n",
    "object_yolo_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a858eb2",
   "metadata": {},
   "source": [
    "### Create txt file for each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80ab300",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yolo_bbox_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_id, image_bboxes_df \u001b[38;5;129;01min\u001b[39;00m \u001b[43myolo_bbox_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     image_bboxes_df\u001b[38;5;241m.\u001b[39mto_csv(object_yolo_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yolo_bbox_df' is not defined"
     ]
    }
   ],
   "source": [
    "for image_id, image_bboxes_df in yolo_bbox_df.groupby('image_id'):\n",
    "    image_bboxes_df.to_csv(object_yolo_path / f'{image_id}.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56a725",
   "metadata": {},
   "source": [
    "### Create dataset yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711abeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886fa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# 데이터셋 폴더 경로\n",
    "dataset_path = '/tf/notebook/datasets'\n",
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "valid_path = 'valid'\n",
    "\n",
    "# train, test, valid 폴더 생성\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "os.makedirs(valid_path, exist_ok=True)\n",
    "\n",
    "# 데이터셋 파일 리스트 생성\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "# 파일 리스트를 무작위로 섞음\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# 데이터셋을 train:test:valid = 70:15:15로 나눔\n",
    "train_size = int(0.7 * len(file_list))\n",
    "test_size = int(0.15 * len(file_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# 데이터셋 폴더 경로\n",
    "dataset_path = '4block_shoplist'\n",
    "train_path = '4block_shoplist/train'\n",
    "test_path = '4block_shoplist/test'\n",
    "valid_path = '4block_shoplist/valid'\n",
    "\n",
    "# train, test, valid 폴더 생성\n",
    "os.makedirs(train_path + '/images', exist_ok=True)\n",
    "os.makedirs(train_path + '/labels', exist_ok=True)\n",
    "os.makedirs(test_path + '/images', exist_ok=True)\n",
    "os.makedirs(test_path + '/labels', exist_ok=True)\n",
    "os.makedirs(valid_path + '/images', exist_ok=True)\n",
    "os.makedirs(valid_path + '/labels', exist_ok=True)\n",
    "\n",
    "# 데이터셋 파일 리스트 생성\n",
    "file_list = os.listdir(dataset_path)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# 데이터셋을 train:test:valid = 70:15:15로 나눔\n",
    "train_size = int(0.7 * len(file_list))\n",
    "test_size = int(0.15 * len(file_list))\n",
    "\n",
    "# train 데이터 복사\n",
    "for filename in file_list[:train_size]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(train_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(train_path + '/labels', filename.replace('.jpeg', '.txt')))\n",
    "\n",
    "# test 데이터 복사\n",
    "for filename in file_list[train_size:train_size+test_size]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(test_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(test_path + '/labels', filename.replace('.jpeg', '.txt')))\n",
    "\n",
    "# validation 데이터 복사\n",
    "for filename in file_list[train_size+test_size:]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(valid_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(valid_path + '/labels', filename.replace('.jpeg', '.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75001253",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "with open('data.yaml', 'w') as f:\n",
    "    for split in splits:\n",
    "        split_path = (object_yolo_path.parent / split).resolve()\n",
    "        f.write(f'{split}: {split_path}\\n')\n",
    "    \n",
    "    num_labels = len(all_component_labels)\n",
    "\n",
    "    f.write('names:\\n')\n",
    "    for index, label in enumerate(all_component_labels):\n",
    "        f.write(f'  {index}: {label}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4cc7a",
   "metadata": {},
   "source": [
    "# Copy screenshot files to object_yolopath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cce262",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids_to_copy = yolo_bbox_df.index.get_level_values('image_id').unique()\n",
    "\n",
    "def copy_image(image_id):\n",
    "    shutil.copy(screenshots_path / f'{image_id}.jpg', rico_yolo_path / f'{image_id}.jpg')\n",
    "    return True\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    results = list(mp_executor.map(copy_image, image_ids_to_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a642f",
   "metadata": {},
   "source": [
    "# Data Split yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdaec0d",
   "metadata": {},
   "source": [
    "### 데이터셋 path 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f50f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_yolo_path = Path('../datasets/object_yolo/all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f4e78",
   "metadata": {},
   "source": [
    "# Image Id get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_ids = map(lambda p :int(p.stem), object_yolo_path.iterdir())\n",
    "all_image_ids = pd.Series(all_image_ids).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa43fe",
   "metadata": {},
   "source": [
    "# Random array create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bb3d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_image_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_image_ids\u001b[49m)\n\u001b[1;32m      2\u001b[0m seed_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_image_ids' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = len(all_image_ids)\n",
    "seed_id = 0\n",
    "np.ramdom.sedd(seed_id)\n",
    "random_indices = np.random.choice(num_samples, num_samples, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347e5a",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "split_ratio = [0.8, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935acd7",
   "metadata": {},
   "source": [
    "## 랜덤 배열을 비율에 맞추어 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.array(split_ratio).cumsum() * num_samples\n",
    "ind = np.floor(ind).astype(int) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92211b",
   "metadata": {},
   "source": [
    "## split별 id값 얻어내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c917883",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = {\n",
    "    'train': all_image_ids[:ind[0]],\n",
    "    'val': all_image_ids[ind[0]:ind[1]],\n",
    "    'test': all_image_ids[ind[1]:],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1085716",
   "metadata": {},
   "source": [
    "## 각 split 폴더로 복사하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d285bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    object_yolo_split_path = object_yolo_path.parent / split\n",
    "    object_yolo_split_path.mkdir(exist_ok=True)\n",
    "    for image_id in image_ids[split]:\n",
    "        shutil.copy(object_yolo_path / f'{image_id}.jpg', object_yolo_split_path / f'{image_id}.jpg')\n",
    "        shutil.copy(object_yolo_path / f'{image_id}.txt', object_yolo_split_path / f'{image_id}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72083189",
   "metadata": {},
   "source": [
    "# Docker 환경\n",
    "\n",
    "```powershell\n",
    "docker run -dit --name training_yolo --gpus all --ipc=host --mount type=volume,source=rico,destination=/weven/datasets --mount type=volume,source=runs,destination=/weven/runs ultralytics/ultralytics\n",
    "```\n",
    "\n",
    "- `-d` 컨테이너 백그라운드로 실행\n",
    "- `-it` 터미널 연결\n",
    "- `--gpus all` NVIDIA GPU 연결\n",
    "- `--ipc=host` YOLO Docker 환경 추천 세팅\n",
    "    \n",
    "    > IPC mode에서 Shared memory 관련 세팅인데, 자세히 알 필요는 없어 보인다.\n",
    "    > \n",
    "- `--mount type=volume,source=rico,destination=/weven/datasets` 데이터 관련 볼륨 마운트\n",
    "- `--mount type=volume,source=runs,destination=/weven/runs` 학습 관련 볼륨 마운트\n",
    "    - `runs`라는 이름의 볼륨이 없으면 자동으로 생성된다.\n",
    "- `--name training_yolo` 컨테이너 이름\n",
    "- `ultralytics/ultralytics` YOLOv8 이미지\n",
    "\n",
    "# 작업 디렉토리\n",
    "\n",
    "- `/weven/datasets` 는 `rico` 라는 이름의 Docker volume\n",
    "    - RICO 데이터셋과 전처리 결과가 남아 있다.\n",
    "- `/weven/runs` 는 `runs` 라는 이름의 Docker volume\n",
    "    - 학습 세팅과 결과가 저장될 볼륨\n",
    "\n",
    "```powershell\n",
    "weven\n",
    "├── datasets\n",
    "│   ├── RICO\n",
    "│   └── rico_yolo\n",
    "│       ├── all\n",
    "│       ├── test\n",
    "│       ├── train\n",
    "│       └── val\n",
    "└── runs\n",
    "    ├── configs\n",
    "    │   └── ym_sgd_lr_5e-3_epochs_200.yaml\n",
    "    └── train\n",
    "        └── rico_yolo.yaml\n",
    "```\n",
    "\n",
    "## 학습 데이터셋 연결\n",
    "\n",
    "- `/weven/runs/train` 디렉토리에 `rico_yolo.yaml` 파일을 미리 준비해 놓는다.\n",
    "\n",
    "### rico_yolo.yaml\n",
    "\n",
    "```yaml\n",
    "train: /weven/datasets/rico_yolo/train\n",
    "val: /weven/datasets/rico_yolo/val\n",
    "test: /weven/datasets/rico_yolo/test\n",
    "names:\n",
    "  0: Advertisement\n",
    "  1: Background Image\n",
    "  2: Bottom Navigation\n",
    "  3: Button Bar\n",
    "  4: Card\n",
    "  5: Checkbox\n",
    "  6: Date Picker\n",
    "  7: Drawer\n",
    "  8: Icon\n",
    "  9: Image\n",
    "  10: Input\n",
    "  11: List Item\n",
    "  12: Map View\n",
    "  13: Modal\n",
    "  14: Multi-Tab\n",
    "  15: Number Stepper\n",
    "  16: On/Off Switch\n",
    "  17: Pager Indicator\n",
    "  18: Radio Button\n",
    "  19: Slider\n",
    "  20: Text\n",
    "  21: Text Button\n",
    "  22: Toolbar\n",
    "  23: Video\n",
    "  24: Web View\n",
    "```\n",
    "\n",
    "# YOLOv8 Training Configs\n",
    "\n",
    "- 학습 세팅을 설정 파일 하나로 조절할 수 있어 매우 편하다.\n",
    "- `/weven/runs/configs` 에 학습 세팅을 저장해둔다.\n",
    "\n",
    "### ym_sgd_lr_5e-3_epochs_200.yaml\n",
    "\n",
    "```yaml\n",
    "task: detect\n",
    "mode: train\n",
    "model: yolov8m.pt\n",
    "data: rico_yolo.yaml\n",
    "epochs: 200\n",
    "patience: 3\n",
    "batch: 16\n",
    "workers: 16\n",
    "project: yolov8m\n",
    "name: sgd_lr_5e-3_epochs_200\n",
    "optimizer: SGD\n",
    "lr0: 0.005\n",
    "```\n",
    "\n",
    "- 학습 결과 저장: `/weven/runs/train/{project}/{name}`\n",
    "\n",
    "# Training\n",
    "\n",
    "- 실행 위치: `/weven/runs/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925665e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3788554442.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    yolo task=detect mode=train model=yolov8m.pt data=/tf/notebook/datasets/data.yaml epochs=100 imgsz=640\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "yolo task=detect mode=train model=yolov8m.pt data=/tf/notebook/datasets/data.yaml epochs=100 imgsz=640 batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146e9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.138 available 😃 Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mepoch\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['epochs=100'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-2e827445-0042-4623-9959-9c1b5985f744.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[19], line 5\u001b[0m\n    results = model.train(data='data.yaml',\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/model.py:368\u001b[0m in \u001b[1;35mtrain\u001b[0m\n    self.trainer = TASK_MAP[self.task][1](overrides=overrides, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/trainer.py:82\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/cfg/__init__.py:114\u001b[0m in \u001b[1;35mget_cfg\u001b[0m\n    check_cfg_mismatch(cfg, overrides)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/cfg/__init__.py:187\u001b[0;36m in \u001b[0;35mcheck_cfg_mismatch\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mepoch\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['epochs=100'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-2e827445-0042-4623-9959-9c1b5985f744.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "results = model.train(data='data.yaml', \n",
    "                      epoch=100,\n",
    "                     imgsz=640,\n",
    "                     batch=16,\n",
    "                     name='yolov8m_test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcda2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to yolov8l.pt...\n",
      "100%|██████████████████████████████████████| 83.7M/83.7M [00:04<00:00, 20.6MB/s]\n",
      "Ultralytics YOLOv8.0.138 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/tf/notebook/datasets/data.yaml, epochs=50, patience=50, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train14\n",
      "Overriding model.yaml nc=80 with nc=19\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5597449  ultralytics.nn.modules.head.Detect           [19, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43644489 parameters, 43644473 gradients, 165.5 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train14', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /tf/notebook/datasets/train/labels.cache... 1020 images, 0 backg\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/notebook/datasets/valid/labels.cache... 200 images, 0 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/train14/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000435, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50      3.28G     0.5917      1.257     0.9718        351        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.637      0.556      0.531      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50      3.33G     0.3835     0.3318     0.8447        197        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.788      0.596      0.651      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50      3.34G     0.3665     0.3057     0.8348        303        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.895      0.667       0.75      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50      3.34G     0.3466     0.2805     0.8313        341        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.89      0.688       0.74       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50      3.33G     0.3416      0.273     0.8301        286        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.954      0.713      0.774      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50      3.33G     0.3158     0.2436     0.8232        427        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.704      0.804      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50      3.33G     0.2952     0.2316     0.8208        382        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.716      0.809      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50      3.33G      0.298      0.233     0.8215        286        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.96      0.707      0.803       0.62\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50      3.34G     0.2817     0.2227     0.8181        170        320: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.708      0.827      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50      3.33G     0.2714     0.2105     0.8162        295        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.986      0.708      0.805      0.608\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50      3.33G     0.2574     0.2006     0.8087        261        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.713      0.822      0.661\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50      3.33G     0.2499     0.1992     0.8116        287        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.976      0.718      0.816      0.654\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50      3.33G     0.2491     0.1954     0.8085        263        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.957      0.707      0.818      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50      3.33G     0.2384     0.1908     0.8065        448        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.977        0.7      0.831      0.658\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50      3.33G      0.241     0.1912     0.8081        303        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.978      0.708      0.828      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50      3.33G     0.2359     0.1855     0.8084        364        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.712      0.836      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50      3.33G     0.2413     0.1858     0.8071        317        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.674      0.834      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50      3.34G     0.2259     0.1794     0.8057        335        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.706      0.841      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50      3.33G      0.224     0.1772     0.8089        302        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.993      0.691      0.852      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50      3.34G     0.2176      0.172     0.8029        346        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991      0.709      0.851      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50      3.33G     0.2162     0.1704     0.8046        308        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.688      0.843        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50      3.33G       0.21      0.169     0.8021        338        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.701      0.839       0.69\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50      3.33G     0.2093     0.1688     0.8089        259        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.997      0.712      0.857      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50      3.34G     0.2068     0.1684     0.8037        292        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.703      0.851      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50      3.33G     0.2023     0.1655     0.8029        294        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.925      0.722      0.846      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50      3.33G     0.1953     0.1611     0.8061        288        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.988      0.701      0.843      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50      3.33G     0.2012      0.158     0.8041        257        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.713      0.849      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50      3.33G     0.1915     0.1526     0.8026        284        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.994      0.712       0.84      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50      3.33G     0.1909     0.1519      0.802        275        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.996      0.701      0.843      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50      3.33G     0.1887     0.1475     0.7986        387        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.995      0.697      0.851      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50      3.34G     0.1841     0.1452     0.7982        249        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.998      0.713      0.853      0.745\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50      3.34G     0.1801     0.1469     0.7988        260        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99      0.708      0.846      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50      3.33G     0.1769     0.1458     0.7936        394        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991      0.708      0.848      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      34/50      3.34G     0.1742     0.1418     0.7968        459        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989       0.69      0.846      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50      3.34G     0.1722     0.1385     0.7955        367        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.988      0.707      0.849      0.726\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50      3.34G     0.1726     0.1409     0.7989        305        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.705      0.845      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50      3.34G     0.1721     0.1399     0.7959        288        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.996      0.712      0.854      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50      3.33G      0.169     0.1355      0.791        364        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989      0.709      0.851       0.73\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50      3.34G     0.1671     0.1345     0.7967        245        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.993       0.71      0.854      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50      3.34G     0.1667      0.134     0.7982        263        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.96      0.716      0.847       0.74\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50      3.34G     0.1904     0.1331     0.7714        197        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99      0.688      0.845      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50      3.33G     0.1896     0.1275      0.773        152        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.978      0.705      0.844      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50      3.33G     0.1895     0.1278     0.7655        151        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.709      0.849      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50      3.34G      0.181     0.1241     0.7667         60        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991       0.71      0.851      0.742\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50      3.33G     0.1786     0.1213       0.77        198        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.704      0.853      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50      3.34G     0.1755     0.1197     0.7671        128        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.705      0.847      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50      3.33G     0.1714     0.1139     0.7683        152        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.986      0.697      0.851      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50      3.33G     0.1655     0.1112     0.7644        129        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989      0.707      0.853      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50      3.33G     0.1627       0.11     0.7677        244        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99        0.7      0.854       0.73\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50      3.34G     0.1639     0.1107     0.7667        175        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.708      0.849      0.735\n",
      "\n",
      "50 epochs completed in 0.204 hours.\n",
      "Optimizer stripped from runs/detect/train14/weights/last.pt, 87.6MB\n",
      "Optimizer stripped from runs/detect/train14/weights/best.pt, 87.6MB\n",
      "\n",
      "Validating runs/detect/train14/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.138 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "Model summary (fused): 268 layers, 43621257 parameters, 0 gradients, 164.9 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.998      0.713      0.853      0.745\n",
      "     typo_price_won_kr        200        600          1      0.809      0.909      0.736\n",
      "           image_photo        200        919      0.993          1      0.994      0.994\n",
      "            btn_square        200       1200          1      0.331      0.656      0.506\n",
      "Speed: 0.2ms preprocess, 8.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo train data=/tf/notebook/datasets/data.yaml model=yolov8l.pt epochs=50 lr0=0.01 imgsz=320\n",
    "batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdbce16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.8/dist-packages (8.0.132)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.0.138-py3-none-any.whl (605 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (3.7.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.26.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.6)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib>=3.2.2->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
      "Installing collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.0.132\n",
      "    Uninstalling ultralytics-8.0.132:\n",
      "      Successfully uninstalled ultralytics-8.0.132\n",
      "Successfully installed ultralytics-8.0.138\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2f95662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.138 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "Model summary (fused): 268 layers, 43621257 parameters, 0 gradients, 164.9 GFLOPs\n",
      "\n",
      "image 1/1 /tf/notebook/202307201547200720.jpeg: 192x320 7 typo_price_won_krs, 4 image_photos, 2 btn_squares, 54.4ms\n",
      "Speed: 0.8ms preprocess, 54.4ms inference, 1.7ms postprocess per image at shape (1, 3, 192, 320)\n",
      "Results saved to \u001b[1mruns/detect/predict16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo detect predict model=/tf/notebook/runs/detect/train14/weights/best.pt source='/tf/notebook/202307201547200720.jpeg' conf=0.1 iou=0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd2d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b8cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_preprocess2.ipynb\t\t        study\r\n",
      " datasets\t\t\t        test.jpg\r\n",
      " efficientlearning\t\t        tf\r\n",
      " efficientnetv2_impurity_del_v1.ipynb   tfhub_split_datasets.ipynb\r\n",
      " labelingtest\t\t\t        traindata_Image_augmentation.ipynb\r\n",
      " logo.v1i.yolov8\t\t        weblogo.v1i.yolov8\r\n",
      " logo.v4i.yolov8\t\t        weblogo.v2i.yolov8\r\n",
      " logs\t\t\t\t       'weblogo.v2i.yolov8 (1)'\r\n",
      " messageImage_1689833556852.jpg         yolov8_header.ipynb\r\n",
      " runs\t\t\t\t        yolov8m.pt\r\n",
      " step1\t\t\t\t        yolov8n.pt\r\n",
      " step2\t\t\t\t        연습용끄적임.ipynb\r\n",
      " step2_preprocessing.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320d73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
