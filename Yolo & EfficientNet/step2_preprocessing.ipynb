{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fb6a8a",
   "metadata": {},
   "source": [
    "# Ultralytics YOLO format\n",
    "\n",
    "## 1. Directory\n",
    "\n",
    "```powershell\n",
    "<Custom Dataset Directory>\n",
    "****â”œâ”€â”€ 0.jpg\n",
    "â”œâ”€â”€ 0.txt\n",
    "â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "### Split Dataset Directory\n",
    "\n",
    "```powershell\n",
    "rico_yolo\n",
    "â”œâ”€â”€ test\n",
    "â”‚   â”œâ”€â”€ 0.jpg\n",
    "â”‚   â”œâ”€â”€ 0.txt\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ train\n",
    "â”‚   â”œâ”€â”€ 2.jpg\n",
    "â”‚   â”œâ”€â”€ 2.txt\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ val\n",
    "    â”œâ”€â”€ 36.jpg\n",
    "    â”œâ”€â”€ 36.txt\n",
    "    â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "## 2. Annotation\n",
    "\n",
    "```\n",
    "<object-class> <x> <y> <width> <height>\n",
    "```\n",
    "\n",
    "### ì˜ˆì‹œ) 2.txt\n",
    "\n",
    "13 0.5 0.4833984375 0.7944444444444444 0.626171875\n",
    "3 0.5 0.758203125 0.7944444444444444 0.0765625\n",
    "21 0.7875 0.758203125 0.16111111111111112 0.065625\n",
    "6 0.5 0.4451171875 0.7944444444444444 0.549609375\n",
    "21 0.5 0.198046875 0.6777777777777778 0.05546875\n",
    "21 0.35381944444444446 0.2568359375 0.3854166666666667 0.062109375\n",
    "8 0.2013888888888889 0.358984375 0.11666666666666667 0.065625\n",
    "8 0.7986111111111112 0.358984375 0.11666666666666667 0.065625\n",
    "\n",
    "## Dataset File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30baae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_text = \"\"\"\n",
    "path: /weven/datasets/\n",
    "train: /weven/datasets/Object/train\n",
    "val: /weven/datasets/rico_yolo/val\n",
    "test: /weven/datasets/rico_yolo/test\n",
    "\n",
    "names:\n",
    "    0: 'typo_text',\n",
    "    1: 'typo_price_num',\n",
    "    2: 'typo_price_dollar',\n",
    "    3: 'typo_price_W',\n",
    "    4: 'typo_price_won_en',\n",
    "    5: 'typo_price_won_kr',\n",
    "    6: 'image_photo',\n",
    "    7: 'image_colorBG',\n",
    "    8: 'image_removeBG',\n",
    "    9: 'icon_arrow_left',\n",
    "    10: 'icon_arrow_top',\n",
    "    11: 'icon_arrow_bottom',\n",
    "    12: 'icon_arrow_right',\n",
    "    13: 'icon_video_play',\n",
    "    14: 'icon_SNS_insta',\n",
    "    15: 'icon_SNS_youtube',\n",
    "    16: 'btn_radius',\n",
    "    17: 'btn_ellipse',\n",
    "    18: 'btn_square'\"\"\"\n",
    "\n",
    "with open('/tf/notebook/datasets/data.yaml', 'w') as file:\n",
    "    file.write(yaml_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8095c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ì‘ì—… ë””ë ‰í† ë¦¬\n",
    "\n",
    "'''powershell\n",
    "weven\n",
    "â”œâ”€â”€ datasets\n",
    "â”‚   â””â”€â”€ Obejectblock\n",
    "â””â”€â”€ notebooks\n",
    "    â””â”€â”€ read_rico_dataset.ipynb\n",
    "\n",
    "'''\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae18f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Training - Preparing  Dataset for YOLO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import functools\n",
    "import itertools\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a94a1",
   "metadata": {},
   "source": [
    "## For parallel computing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "num_workers = 32  # The number of threads or processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6f8b7",
   "metadata": {},
   "source": [
    "# mt_executor = concurrent.futures.ThreadPoolExecutor(num_workers)  \n",
    "# Use ThreadPoolExecutor for CPU-bound tasks.\n",
    "# mp_executor = concurrent.futures.ProcessPoolExecutor(num_workers)  \n",
    "# Use ProcessPoolExecutor for I/O-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3547d",
   "metadata": {},
   "source": [
    "#  dataset path settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshots_path = Path('../datasets/Object/~')  # 66261 '.jpg' and '.json'\n",
    "annotations_path = Path('../datasets/Object/~/')  # 66261 '.png' and 'json'\n",
    "\n",
    "image_ids = pd.Series([int(p.stem) for p in screenshots_path.iterdir() if p.suffix == '.json'], name='image_id')\n",
    "image_ids = image_ids.sort_values().reset_index(drop='index')\n",
    "\n",
    "screenshot_paths = [screenshots_path / f'{image_id}.jpg' for image_id in image_ids]\n",
    "annotation_paths = [annotations_path / f'{image_id}.png' for image_id in image_ids]\n",
    "view_hierarchy_json_paths = [screenshots_path / f'{image_id}.json' for image_id in image_ids]\n",
    "annotation_hierarchy_json_paths = [annotations_path / f'{image_id}.json' for image_id in image_ids]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965b090",
   "metadata": {},
   "source": [
    "## Read hierarchy json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92986973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hierarchy_from_json(json_path):\n",
    "    with open(json_path) as fp:\n",
    "        hierarchy = json.load(fp)\n",
    "        return hierarchy\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    annotation_hierarchies = mp_executor.map(get_hierarchy_from_json, annotation_hierarchy_json_paths)\n",
    "    annotation_hierarchies = list(annotation_hierarchies)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    view_hierarchies = mp_executor.map(get_hierarchy_from_json, view_hierarchy_json_paths)\n",
    "    view_hierarchies = list(view_hierarchies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c9f0e",
   "metadata": {},
   "source": [
    "# Get Image sizes (ê°€ë¡œ, ì„¸ë¡œ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imsize(path):\n",
    "    im = Image.open(path)\n",
    "    return im.size\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    screenshot_sizes = mp_executor.map(get_imsize, screenshot_paths)\n",
    "    screenshot_sizes = list(screenshot_sizes)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    annotation_sizes = mp_executor.map(get_imsize, annotation_paths)\n",
    "    annotation_sizes = list(annotation_sizes)\n",
    "\n",
    "screenshot_sizes_df = pd.DataFrame(screenshot_sizes, columns=['width', 'height'])\n",
    "annotation_sizes_df = pd.DataFrame(annotation_sizes, columns=['width', 'height'])\n",
    "\n",
    "screenshot_sizes_df.insert(0, 'image_id', image_ids)\n",
    "annotation_sizes_df.insert(0, 'image_id', image_ids)\n",
    "screenshot_sizes_df['ratio'] = screenshot_sizes_df['width'] / screenshot_sizes_df['height']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba561a",
   "metadata": {},
   "source": [
    "# Get useful information from hierarchies\n",
    "\n",
    "## A. From view_hierarchies\n",
    "\n",
    "### 1. root size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_size(hierarchy):\n",
    "    return hierarchy['activity']['root']['rel-bounds']\n",
    "\n",
    "root_sizes = map(get_root_size, view_hierarchies)\n",
    "\n",
    "root_sizes_df = pd.DataFrame(root_sizes, columns=['x1', 'y1', 'x2', 'y2'])\n",
    "root_sizes_df.insert(0, 'image_id', image_ids)\n",
    "root_sizes_df['ratio'] = root_sizes_df['x2'] / root_sizes_df['y2']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cb7ef",
   "metadata": {},
   "source": [
    "### 2. root classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_classes(hierarchy):\n",
    "    root_class_name = hierarchy['activity']['root']['class']\n",
    "    return root_class_name\n",
    "\n",
    "root_class_names = list(map(get_root_classes, view_hierarchies))\n",
    "pd.Series(root_class_names).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a546a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## B. From annotaion_hierarchies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_attributes(hierarchy, attribute_name):\n",
    "    uniques = set()\n",
    "\n",
    "    recursive_function = functools.partial(get_unique_attributes, attribute_name=attribute_name)\n",
    "    if type(hierarchy) is list:\n",
    "        uniques_list = map(recursive_function, hierarchy)\n",
    "        uniques |= functools.reduce(lambda x,y: x | y, uniques_list, set())\n",
    "    \n",
    "    if type(hierarchy) is dict:\n",
    "        if attribute_name in hierarchy:\n",
    "            uniques.add(hierarchy[attribute_name])\n",
    "    \n",
    "        if 'children' in hierarchy:\n",
    "            uniques |= recursive_function(hierarchy['children'])\n",
    "    \n",
    "    return uniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c9f07",
   "metadata": {},
   "source": [
    "### 1. componentLabel, 2. iconClass, 3.textButtonClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_component_labels = get_unique_attributes(annotation_hierarchies, 'componentLabel')\n",
    "icon_classes = get_unique_attributes(annotation_hierarchies, 'iconClass')\n",
    "text_button_classes = get_unique_attributes(annotation_hierarchies, 'textButtonClass')\n",
    "\n",
    "all_component_labels = sorted(list(all_component_labels))\n",
    "icon_classes = sorted(list(icon_classes))\n",
    "text_button_classes = sorted(list(text_button_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fd2a3",
   "metadata": {},
   "source": [
    "# Get bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4dad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_and_bounds(hierarchy):\n",
    "    bboxes = []\n",
    "\n",
    "    if 'componentLabel' in hierarchy:\n",
    "        label = hierarchy['componentLabel']\n",
    "        x1, y1, x2, y2 = hierarchy['bounds']\n",
    "        bboxes.append(dict(label=label, x1=x1, y1=y1, x2=x2, y2=y2))\n",
    "\n",
    "    if 'children' in hierarchy:\n",
    "        bboxes += list(itertools.chain.from_iterable(map(get_component_and_bounds, hierarchy['children'])))\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "all_bboxes = list(map(get_component_and_bounds, annotation_hierarchies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b42ff4",
   "metadata": {},
   "source": [
    "# Preprocess dataset\n",
    "\n",
    "### Drop duplicated boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf55a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_bbox_dfs = list(map(lambda x: pd.DataFrame(x).drop_duplicates().reset_index(drop=True), all_bboxes))\n",
    "\n",
    "\n",
    "\n",
    "all_bbox_df = pd.concat(all_bbox_dfs, keys=image_ids, names=['image_id', 'bbox_id'])\n",
    "all_bbox_df = all_bbox_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd7605",
   "metadata": {},
   "source": [
    "### **Find landscape view (wrong matches)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f19889",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape_screenshot_df = screenshot_sizes_df[screenshot_sizes_df['ratio'] > 1]\n",
    "landscaped = all_bbox_df.index.isin(landscape_screenshot_df['image_id'], level='image_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e5b34",
   "metadata": {},
   "source": [
    "### **Find overflowed boxes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotation_sizes = {'width': 1440, 'height': 2560}\n",
    "overflowed = all_bbox_df['x1'] < 0\n",
    "overflowed |= all_bbox_df['y1'] < 0\n",
    "overflowed |= all_bbox_df['x2'] > annotation_sizes['width']\n",
    "overflowed |= all_bbox_df['y2'] > annotation_sizes['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac387a",
   "metadata": {},
   "source": [
    "### **Find boxes with 0 or negative values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c381804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negated = all_bbox_df['x1'] >= all_bbox_df['x2']\n",
    "negated |= all_bbox_df['y1'] >= all_bbox_df['y2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a672cdf",
   "metadata": {},
   "source": [
    "### **Filter out all corrupted boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b22690",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df = all_bbox_df[~(landscaped | overflowed | negated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa41488",
   "metadata": {},
   "source": [
    "# **Convert to YOLO format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = pd.Series(list(all_component_labels))\n",
    "class_index = pd.Series(class_index.index.values, index=class_index)\n",
    "\n",
    "\n",
    "\n",
    "yolo_bbox_df = pd.DataFrame({\n",
    "    'label': pd.Series(class_index[bbox_df['label']].values, index=bbox_df.index),\n",
    "    'x': (bbox_df['x1'] + bbox_df['x2']) / 2 / annotation_sizes['width'],\n",
    "    'y': (bbox_df['y1'] + bbox_df['y2']) / 2 / annotation_sizes['height'],\n",
    "    'width': (bbox_df['x2'] - bbox_df['x1']) / annotation_sizes['width'],\n",
    "    'height': (bbox_df['y2'] - bbox_df['y1']) / annotation_sizes['height'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc7c2d",
   "metadata": {},
   "source": [
    "# Create YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22130d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_yolo_path = Path('tf/datasets/datasets')\n",
    "object_yolo_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a858eb2",
   "metadata": {},
   "source": [
    "### Create txt file for each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80ab300",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yolo_bbox_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_id, image_bboxes_df \u001b[38;5;129;01min\u001b[39;00m \u001b[43myolo_bbox_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     image_bboxes_df\u001b[38;5;241m.\u001b[39mto_csv(object_yolo_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yolo_bbox_df' is not defined"
     ]
    }
   ],
   "source": [
    "for image_id, image_bboxes_df in yolo_bbox_df.groupby('image_id'):\n",
    "    image_bboxes_df.to_csv(object_yolo_path / f'{image_id}.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56a725",
   "metadata": {},
   "source": [
    "### Create dataset yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711abeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886fa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ\n",
    "dataset_path = '/tf/notebook/datasets'\n",
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "valid_path = 'valid'\n",
    "\n",
    "# train, test, valid í´ë” ìƒì„±\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "os.makedirs(valid_path, exist_ok=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "# íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ train:test:valid = 70:15:15ë¡œ ë‚˜ëˆ”\n",
    "train_size = int(0.7 * len(file_list))\n",
    "test_size = int(0.15 * len(file_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ\n",
    "dataset_path = '4block_shoplist'\n",
    "train_path = '4block_shoplist/train'\n",
    "test_path = '4block_shoplist/test'\n",
    "valid_path = '4block_shoplist/valid'\n",
    "\n",
    "# train, test, valid í´ë” ìƒì„±\n",
    "os.makedirs(train_path + '/images', exist_ok=True)\n",
    "os.makedirs(train_path + '/labels', exist_ok=True)\n",
    "os.makedirs(test_path + '/images', exist_ok=True)\n",
    "os.makedirs(test_path + '/labels', exist_ok=True)\n",
    "os.makedirs(valid_path + '/images', exist_ok=True)\n",
    "os.makedirs(valid_path + '/labels', exist_ok=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "file_list = os.listdir(dataset_path)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ train:test:valid = 70:15:15ë¡œ ë‚˜ëˆ”\n",
    "train_size = int(0.7 * len(file_list))\n",
    "test_size = int(0.15 * len(file_list))\n",
    "\n",
    "# train ë°ì´í„° ë³µì‚¬\n",
    "for filename in file_list[:train_size]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(train_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(train_path + '/labels', filename.replace('.jpeg', '.txt')))\n",
    "\n",
    "# test ë°ì´í„° ë³µì‚¬\n",
    "for filename in file_list[train_size:train_size+test_size]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(test_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(test_path + '/labels', filename.replace('.jpeg', '.txt')))\n",
    "\n",
    "# validation ë°ì´í„° ë³µì‚¬\n",
    "for filename in file_list[train_size+test_size:]:\n",
    "    if filename.endswith('.jpeg'):\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        txt_path = os.path.join(dataset_path, filename.replace('.jpeg', '.txt'))\n",
    "        shutil.copy(image_path, os.path.join(valid_path + '/images', filename))\n",
    "        shutil.copy(txt_path, os.path.join(valid_path + '/labels', filename.replace('.jpeg', '.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75001253",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "with open('data.yaml', 'w') as f:\n",
    "    for split in splits:\n",
    "        split_path = (object_yolo_path.parent / split).resolve()\n",
    "        f.write(f'{split}: {split_path}\\n')\n",
    "    \n",
    "    num_labels = len(all_component_labels)\n",
    "\n",
    "    f.write('names:\\n')\n",
    "    for index, label in enumerate(all_component_labels):\n",
    "        f.write(f'  {index}: {label}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4cc7a",
   "metadata": {},
   "source": [
    "# Copy screenshot files to object_yolopath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cce262",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids_to_copy = yolo_bbox_df.index.get_level_values('image_id').unique()\n",
    "\n",
    "def copy_image(image_id):\n",
    "    shutil.copy(screenshots_path / f'{image_id}.jpg', rico_yolo_path / f'{image_id}.jpg')\n",
    "    return True\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(num_workers) as mp_executor:\n",
    "    results = list(mp_executor.map(copy_image, image_ids_to_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a642f",
   "metadata": {},
   "source": [
    "# Data Split yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdaec0d",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ path ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f50f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_yolo_path = Path('../datasets/object_yolo/all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f4e78",
   "metadata": {},
   "source": [
    "# Image Id get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_ids = map(lambda p :int(p.stem), object_yolo_path.iterdir())\n",
    "all_image_ids = pd.Series(all_image_ids).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa43fe",
   "metadata": {},
   "source": [
    "# Random array create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bb3d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_image_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_image_ids\u001b[49m)\n\u001b[1;32m      2\u001b[0m seed_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_image_ids' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = len(all_image_ids)\n",
    "seed_id = 0\n",
    "np.ramdom.sedd(seed_id)\n",
    "random_indices = np.random.choice(num_samples, num_samples, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347e5a",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "split_ratio = [0.8, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935acd7",
   "metadata": {},
   "source": [
    "## ëœë¤ ë°°ì—´ì„ ë¹„ìœ¨ì— ë§ì¶”ì–´ ë‚˜ëˆ„ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.array(split_ratio).cumsum() * num_samples\n",
    "ind = np.floor(ind).astype(int) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92211b",
   "metadata": {},
   "source": [
    "## splitë³„ idê°’ ì–»ì–´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c917883",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = {\n",
    "    'train': all_image_ids[:ind[0]],\n",
    "    'val': all_image_ids[ind[0]:ind[1]],\n",
    "    'test': all_image_ids[ind[1]:],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1085716",
   "metadata": {},
   "source": [
    "## ê° split í´ë”ë¡œ ë³µì‚¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d285bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    object_yolo_split_path = object_yolo_path.parent / split\n",
    "    object_yolo_split_path.mkdir(exist_ok=True)\n",
    "    for image_id in image_ids[split]:\n",
    "        shutil.copy(object_yolo_path / f'{image_id}.jpg', object_yolo_split_path / f'{image_id}.jpg')\n",
    "        shutil.copy(object_yolo_path / f'{image_id}.txt', object_yolo_split_path / f'{image_id}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72083189",
   "metadata": {},
   "source": [
    "# Docker í™˜ê²½\n",
    "\n",
    "```powershell\n",
    "docker run -dit --name training_yolo --gpus all --ipc=host --mount type=volume,source=rico,destination=/weven/datasets --mount type=volume,source=runs,destination=/weven/runs ultralytics/ultralytics\n",
    "```\n",
    "\n",
    "- `-d` ì»¨í…Œì´ë„ˆ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰\n",
    "- `-it` í„°ë¯¸ë„ ì—°ê²°\n",
    "- `--gpus all` NVIDIA GPU ì—°ê²°\n",
    "- `--ipc=host` YOLO Docker í™˜ê²½ ì¶”ì²œ ì„¸íŒ…\n",
    "    \n",
    "    > IPC modeì—ì„œ Shared memory ê´€ë ¨ ì„¸íŒ…ì¸ë°, ìì„¸íˆ ì•Œ í•„ìš”ëŠ” ì—†ì–´ ë³´ì¸ë‹¤.\n",
    "    > \n",
    "- `--mount type=volume,source=rico,destination=/weven/datasets` ë°ì´í„° ê´€ë ¨ ë³¼ë¥¨ ë§ˆìš´íŠ¸\n",
    "- `--mount type=volume,source=runs,destination=/weven/runs` í•™ìŠµ ê´€ë ¨ ë³¼ë¥¨ ë§ˆìš´íŠ¸\n",
    "    - `runs`ë¼ëŠ” ì´ë¦„ì˜ ë³¼ë¥¨ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±ëœë‹¤.\n",
    "- `--name training_yolo` ì»¨í…Œì´ë„ˆ ì´ë¦„\n",
    "- `ultralytics/ultralytics` YOLOv8 ì´ë¯¸ì§€\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬\n",
    "\n",
    "- `/weven/datasets` ëŠ” `rico` ë¼ëŠ” ì´ë¦„ì˜ Docker volume\n",
    "    - RICO ë°ì´í„°ì…‹ê³¼ ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë‚¨ì•„ ìˆë‹¤.\n",
    "- `/weven/runs` ëŠ” `runs` ë¼ëŠ” ì´ë¦„ì˜ Docker volume\n",
    "    - í•™ìŠµ ì„¸íŒ…ê³¼ ê²°ê³¼ê°€ ì €ì¥ë  ë³¼ë¥¨\n",
    "\n",
    "```powershell\n",
    "weven\n",
    "â”œâ”€â”€ datasets\n",
    "â”‚   â”œâ”€â”€ RICO\n",
    "â”‚   â””â”€â”€ rico_yolo\n",
    "â”‚       â”œâ”€â”€ all\n",
    "â”‚       â”œâ”€â”€ test\n",
    "â”‚       â”œâ”€â”€ train\n",
    "â”‚       â””â”€â”€ val\n",
    "â””â”€â”€ runs\n",
    "    â”œâ”€â”€ configs\n",
    "    â”‚   â””â”€â”€ ym_sgd_lr_5e-3_epochs_200.yaml\n",
    "    â””â”€â”€ train\n",
    "        â””â”€â”€ rico_yolo.yaml\n",
    "```\n",
    "\n",
    "## í•™ìŠµ ë°ì´í„°ì…‹ ì—°ê²°\n",
    "\n",
    "- `/weven/runs/train` ë””ë ‰í† ë¦¬ì— `rico_yolo.yaml` íŒŒì¼ì„ ë¯¸ë¦¬ ì¤€ë¹„í•´ ë†“ëŠ”ë‹¤.\n",
    "\n",
    "### rico_yolo.yaml\n",
    "\n",
    "```yaml\n",
    "train: /weven/datasets/rico_yolo/train\n",
    "val: /weven/datasets/rico_yolo/val\n",
    "test: /weven/datasets/rico_yolo/test\n",
    "names:\n",
    "  0: Advertisement\n",
    "  1: Background Image\n",
    "  2: Bottom Navigation\n",
    "  3: Button Bar\n",
    "  4: Card\n",
    "  5: Checkbox\n",
    "  6: Date Picker\n",
    "  7: Drawer\n",
    "  8: Icon\n",
    "  9: Image\n",
    "  10: Input\n",
    "  11: List Item\n",
    "  12: Map View\n",
    "  13: Modal\n",
    "  14: Multi-Tab\n",
    "  15: Number Stepper\n",
    "  16: On/Off Switch\n",
    "  17: Pager Indicator\n",
    "  18: Radio Button\n",
    "  19: Slider\n",
    "  20: Text\n",
    "  21: Text Button\n",
    "  22: Toolbar\n",
    "  23: Video\n",
    "  24: Web View\n",
    "```\n",
    "\n",
    "# YOLOv8 Training Configs\n",
    "\n",
    "- í•™ìŠµ ì„¸íŒ…ì„ ì„¤ì • íŒŒì¼ í•˜ë‚˜ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆì–´ ë§¤ìš° í¸í•˜ë‹¤.\n",
    "- `/weven/runs/configs` ì— í•™ìŠµ ì„¸íŒ…ì„ ì €ì¥í•´ë‘”ë‹¤.\n",
    "\n",
    "### ym_sgd_lr_5e-3_epochs_200.yaml\n",
    "\n",
    "```yaml\n",
    "task: detect\n",
    "mode: train\n",
    "model: yolov8m.pt\n",
    "data: rico_yolo.yaml\n",
    "epochs: 200\n",
    "patience: 3\n",
    "batch: 16\n",
    "workers: 16\n",
    "project: yolov8m\n",
    "name: sgd_lr_5e-3_epochs_200\n",
    "optimizer: SGD\n",
    "lr0: 0.005\n",
    "```\n",
    "\n",
    "- í•™ìŠµ ê²°ê³¼ ì €ì¥: `/weven/runs/train/{project}/{name}`\n",
    "\n",
    "# Training\n",
    "\n",
    "- ì‹¤í–‰ ìœ„ì¹˜: `/weven/runs/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925665e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3788554442.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    yolo task=detect mode=train model=yolov8m.pt data=/tf/notebook/datasets/data.yaml epochs=100 imgsz=640\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "yolo task=detect mode=train model=yolov8m.pt data=/tf/notebook/datasets/data.yaml epochs=100 imgsz=640 batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146e9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.138 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mepoch\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['epochs=100'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-2e827445-0042-4623-9959-9c1b5985f744.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[19], line 5\u001b[0m\n    results = model.train(data='data.yaml',\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/model.py:368\u001b[0m in \u001b[1;35mtrain\u001b[0m\n    self.trainer = TASK_MAP[self.task][1](overrides=overrides, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/engine/trainer.py:82\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/cfg/__init__.py:114\u001b[0m in \u001b[1;35mget_cfg\u001b[0m\n    check_cfg_mismatch(cfg, overrides)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/yolo/cfg/__init__.py:187\u001b[0;36m in \u001b[0;35mcheck_cfg_mismatch\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mepoch\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['epochs=100'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-2e827445-0042-4623-9959-9c1b5985f744.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "results = model.train(data='data.yaml', \n",
    "                      epoch=100,\n",
    "                     imgsz=640,\n",
    "                     batch=16,\n",
    "                     name='yolov8m_test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcda2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to yolov8l.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.7M/83.7M [00:04<00:00, 20.6MB/s]\n",
      "Ultralytics YOLOv8.0.138 ğŸš€ Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/tf/notebook/datasets/data.yaml, epochs=50, patience=50, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train14\n",
      "Overriding model.yaml nc=80 with nc=19\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5597449  ultralytics.nn.modules.head.Detect           [19, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43644489 parameters, 43644473 gradients, 165.5 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train14', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /tf/notebook/datasets/train/labels.cache... 1020 images, 0 backg\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/notebook/datasets/valid/labels.cache... 200 images, 0 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/train14/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000435, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50      3.28G     0.5917      1.257     0.9718        351        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.637      0.556      0.531      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50      3.33G     0.3835     0.3318     0.8447        197        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.788      0.596      0.651      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50      3.34G     0.3665     0.3057     0.8348        303        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.895      0.667       0.75      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50      3.34G     0.3466     0.2805     0.8313        341        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.89      0.688       0.74       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50      3.33G     0.3416      0.273     0.8301        286        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.954      0.713      0.774      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50      3.33G     0.3158     0.2436     0.8232        427        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.704      0.804      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50      3.33G     0.2952     0.2316     0.8208        382        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.716      0.809      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50      3.33G      0.298      0.233     0.8215        286        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.96      0.707      0.803       0.62\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50      3.34G     0.2817     0.2227     0.8181        170        320: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.708      0.827      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50      3.33G     0.2714     0.2105     0.8162        295        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.986      0.708      0.805      0.608\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50      3.33G     0.2574     0.2006     0.8087        261        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.713      0.822      0.661\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50      3.33G     0.2499     0.1992     0.8116        287        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.976      0.718      0.816      0.654\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50      3.33G     0.2491     0.1954     0.8085        263        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.957      0.707      0.818      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50      3.33G     0.2384     0.1908     0.8065        448        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.977        0.7      0.831      0.658\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50      3.33G      0.241     0.1912     0.8081        303        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.978      0.708      0.828      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50      3.33G     0.2359     0.1855     0.8084        364        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.712      0.836      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50      3.33G     0.2413     0.1858     0.8071        317        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.674      0.834      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50      3.34G     0.2259     0.1794     0.8057        335        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.706      0.841      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50      3.33G      0.224     0.1772     0.8089        302        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.993      0.691      0.852      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50      3.34G     0.2176      0.172     0.8029        346        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991      0.709      0.851      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50      3.33G     0.2162     0.1704     0.8046        308        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.688      0.843        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50      3.33G       0.21      0.169     0.8021        338        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.701      0.839       0.69\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50      3.33G     0.2093     0.1688     0.8089        259        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.997      0.712      0.857      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50      3.34G     0.2068     0.1684     0.8037        292        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.987      0.703      0.851      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50      3.33G     0.2023     0.1655     0.8029        294        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.925      0.722      0.846      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50      3.33G     0.1953     0.1611     0.8061        288        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.988      0.701      0.843      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50      3.33G     0.2012      0.158     0.8041        257        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.713      0.849      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50      3.33G     0.1915     0.1526     0.8026        284        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.994      0.712       0.84      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50      3.33G     0.1909     0.1519      0.802        275        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.996      0.701      0.843      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50      3.33G     0.1887     0.1475     0.7986        387        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.995      0.697      0.851      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50      3.34G     0.1841     0.1452     0.7982        249        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.998      0.713      0.853      0.745\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50      3.34G     0.1801     0.1469     0.7988        260        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99      0.708      0.846      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50      3.33G     0.1769     0.1458     0.7936        394        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991      0.708      0.848      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      34/50      3.34G     0.1742     0.1418     0.7968        459        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989       0.69      0.846      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50      3.34G     0.1722     0.1385     0.7955        367        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.988      0.707      0.849      0.726\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50      3.34G     0.1726     0.1409     0.7989        305        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.984      0.705      0.845      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50      3.34G     0.1721     0.1399     0.7959        288        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.996      0.712      0.854      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50      3.33G      0.169     0.1355      0.791        364        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989      0.709      0.851       0.73\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50      3.34G     0.1671     0.1345     0.7967        245        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.993       0.71      0.854      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50      3.34G     0.1667      0.134     0.7982        263        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.96      0.716      0.847       0.74\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50      3.34G     0.1904     0.1331     0.7714        197        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99      0.688      0.845      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50      3.33G     0.1896     0.1275      0.773        152        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.978      0.705      0.844      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50      3.33G     0.1895     0.1278     0.7655        151        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.709      0.849      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50      3.34G      0.181     0.1241     0.7667         60        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.991       0.71      0.851      0.742\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50      3.33G     0.1786     0.1213       0.77        198        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.983      0.704      0.853      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50      3.34G     0.1755     0.1197     0.7671        128        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.981      0.705      0.847      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50      3.33G     0.1714     0.1139     0.7683        152        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.986      0.697      0.851      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50      3.33G     0.1655     0.1112     0.7644        129        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.989      0.707      0.853      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50      3.33G     0.1627       0.11     0.7677        244        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719       0.99        0.7      0.854       0.73\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50      3.34G     0.1639     0.1107     0.7667        175        320: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.982      0.708      0.849      0.735\n",
      "\n",
      "50 epochs completed in 0.204 hours.\n",
      "Optimizer stripped from runs/detect/train14/weights/last.pt, 87.6MB\n",
      "Optimizer stripped from runs/detect/train14/weights/best.pt, 87.6MB\n",
      "\n",
      "Validating runs/detect/train14/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.138 ğŸš€ Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "Model summary (fused): 268 layers, 43621257 parameters, 0 gradients, 164.9 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        200       2719      0.998      0.713      0.853      0.745\n",
      "     typo_price_won_kr        200        600          1      0.809      0.909      0.736\n",
      "           image_photo        200        919      0.993          1      0.994      0.994\n",
      "            btn_square        200       1200          1      0.331      0.656      0.506\n",
      "Speed: 0.2ms preprocess, 8.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo train data=/tf/notebook/datasets/data.yaml model=yolov8l.pt epochs=50 lr0=0.01 imgsz=320\n",
    "batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdbce16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.8/dist-packages (8.0.132)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.0.138-py3-none-any.whl (605 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (3.7.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.26.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.6)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib>=3.2.2->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
      "Installing collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.0.132\n",
      "    Uninstalling ultralytics-8.0.132:\n",
      "      Successfully uninstalled ultralytics-8.0.132\n",
      "Successfully installed ultralytics-8.0.138\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2f95662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.138 ğŸš€ Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "Model summary (fused): 268 layers, 43621257 parameters, 0 gradients, 164.9 GFLOPs\n",
      "\n",
      "image 1/1 /tf/notebook/202307201547200720.jpeg: 192x320 7 typo_price_won_krs, 4 image_photos, 2 btn_squares, 54.4ms\n",
      "Speed: 0.8ms preprocess, 54.4ms inference, 1.7ms postprocess per image at shape (1, 3, 192, 320)\n",
      "Results saved to \u001b[1mruns/detect/predict16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo detect predict model=/tf/notebook/runs/detect/train14/weights/best.pt source='/tf/notebook/202307201547200720.jpeg' conf=0.1 iou=0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd2d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b8cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_preprocess2.ipynb\t\t        study\r\n",
      " datasets\t\t\t        test.jpg\r\n",
      " efficientlearning\t\t        tf\r\n",
      " efficientnetv2_impurity_del_v1.ipynb   tfhub_split_datasets.ipynb\r\n",
      " labelingtest\t\t\t        traindata_Image_augmentation.ipynb\r\n",
      " logo.v1i.yolov8\t\t        weblogo.v1i.yolov8\r\n",
      " logo.v4i.yolov8\t\t        weblogo.v2i.yolov8\r\n",
      " logs\t\t\t\t       'weblogo.v2i.yolov8 (1)'\r\n",
      " messageImage_1689833556852.jpg         yolov8_header.ipynb\r\n",
      " runs\t\t\t\t        yolov8m.pt\r\n",
      " step1\t\t\t\t        yolov8n.pt\r\n",
      " step2\t\t\t\t        ì—°ìŠµìš©ë„ì ì„.ipynb\r\n",
      " step2_preprocessing.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320d73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
