{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970aebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f86cc02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DetectionModel' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m weights1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path1)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m weights2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path2)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mweights1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights2\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# averaged_weights = {\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     key: (weights1[key] + weights2[key]) / 2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     for key in weights1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     if weights1[key] is not None and weights2[key] is not None\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DetectionModel' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "model_path1 = '/tf/notebook/ElementDetect.pt'\n",
    "model_path2 = '/tf/notebook/ElementDetect_swiper.pt'\n",
    "\n",
    "weights1 = torch.load(model_path1)['model']\n",
    "weights2 = torch.load(model_path2)['model']\n",
    "\n",
    "print(weights1.keys())\n",
    "print(weights2.keys())\n",
    "\n",
    "# averaged_weights = {\n",
    "#     key: (weights1[key] + weights2[key]) / 2\n",
    "#     for key in weights1\n",
    "#     if weights1[key] is not None and weights2[key] is not None\n",
    "# }\n",
    "averaged_weights = {\n",
    "    key: (weights1[key] + weights2[key]) / 2\n",
    "    for key in weights1.keys()\n",
    "    if isinstance(weights1[key], torch.Tensor) and isinstance(weights2[key], torch.Tensor)\n",
    "}\n",
    "print(averaged_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65996536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chest_imagenome_yolov8_predictions(\n",
    "        model_name_or_path, dicom_id, image_size, conf_thres=0.25, iou_thres=0.45, figsize=(10, 10),\n",
    "    ):\n",
    "\n",
    "    # Load model\n",
    "    from medvqa.models.vision.visual_modules import create_yolov8_model\n",
    "    from medvqa.datasets.chest_imagenome import CHEST_IMAGENOME_NUM_BBOX_CLASSES, CHEST_IMAGENOME_BBOX_NAMES\n",
    "    class_names = {i:x for i, x in enumerate(CHEST_IMAGENOME_BBOX_NAMES)}\n",
    "    model = create_yolov8_model(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        nc=CHEST_IMAGENOME_NUM_BBOX_CLASSES,\n",
    "        class_names=class_names,\n",
    "    )\n",
    "\n",
    "    # Load image\n",
    "    from medvqa.datasets.mimiccxr import get_imageId2PartPatientStudy, get_mimiccxr_large_image_path\n",
    "    imageId2PartPatientStudy = get_imageId2PartPatientStudy()    \n",
    "    part_id, patient_id, study_id = imageId2PartPatientStudy[dicom_id]\n",
    "    image_path = get_mimiccxr_large_image_path(part_id, patient_id, study_id, dicom_id)\n",
    "    print(f'dicom_id = {dicom_id}')\n",
    "    print(f'image_path = {image_path}')\n",
    "    from medvqa.datasets.image_processing import get_image_transform\n",
    "    transform = get_image_transform(\n",
    "        image_size=image_size,\n",
    "        use_bbox_aware_transform=True,\n",
    "    )\n",
    "    image, image_size_before, image_size_after = transform(image_path, return_image_size=True)\n",
    "    print(f'image.shape = {image.shape}')\n",
    "    print(f'image_size_before = {image_size_before}')\n",
    "    print(f'image_size_after = {image_size_after}')\n",
    "\n",
    "    # Run model in inference mode\n",
    "    import torch\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        results = model(image.unsqueeze(0))\n",
    "        assert len(results) == 2\n",
    "        print(f'results[0].shape = {results[0].shape}')\n",
    "        assert len(results[1]) == 3\n",
    "\n",
    "    # Obtain predictions\n",
    "    from ultralytics.yolo.utils.ops import non_max_suppression\n",
    "    import numpy as np\n",
    "    predictions = results[0].detach().cpu()\n",
    "    predictions = non_max_suppression(predictions, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "    print(f'len(predictions) (after NMS) = {len(predictions)}')\n",
    "    predictions = predictions[0].numpy()\n",
    "    print(f'predictions.shape = {predictions.shape}')\n",
    "    pred_coords = predictions[:, :4]\n",
    "    pred_coords /= np.array([image_size_before[1], image_size_before[0], image_size_before[1], image_size_before[0]])\n",
    "    pred_classes = predictions[:, 5].astype(int)\n",
    "    print(f'pred_coords.shape = {pred_coords.shape}')\n",
    "    print(f'pred_classes.shape = {pred_classes.shape}')\n",
    "\n",
    "    # Visualize predictions\n",
    "    from medvqa.datasets.chest_imagenome.chest_imagenome_dataset_management import _visualize_predicted_bounding_boxes__yolo    \n",
    "    _visualize_predicted_bounding_boxes__yolo(\n",
    "        dicom_id=dicom_id,\n",
    "        pred_coords=pred_coords,\n",
    "        pred_classes=pred_classes,\n",
    "        figsize=figsize,\n",
    "        format='xyxy',\n",
    "    )\n",
    "\n",
    "    # Release GPU memory\n",
    "    del model\n",
    "    del image\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\n",
    "        'dicom_id': dicom_id,\n",
    "        'pred_coords': pred_coords,\n",
    "        'pred_classes': pred_classes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4967a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "visualize_chest_imagenome_yolov8_predictions() missing 3 required positional arguments: 'model_name_or_path', 'dicom_id', and 'image_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualize_chest_imagenome_yolov8_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: visualize_chest_imagenome_yolov8_predictions() missing 3 required positional arguments: 'model_name_or_path', 'dicom_id', and 'image_size'"
     ]
    }
   ],
   "source": [
    "model_name_or_path = '/tf/notebook/ElementDetect.pt'\n",
    "\n",
    "image_size = 640\n",
    "visualize_chest_imagenome_yolov8_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f3d286",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "'<ultralytics.models.yolo.model.YOLO object at 0x7fc8f0fae970>' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     final_detections \u001b[38;5;241m=\u001b[39m apply_nms(merged_detections, iou_threshold)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_detections\n\u001b[0;32m----> 8\u001b[0m detections_model1 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tf/notebook/ElementDetect.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/notebook/1231.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m detections_model2 \u001b[38;5;241m=\u001b[39m extract_detections(YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tf/notebook/ElementDetect_swiper.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m), image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tf/notebook/1231.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m final_detections \u001b[38;5;241m=\u001b[39m merge_detections(detections_model1, detections_model2)\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mextract_detections\u001b[0;34m(model_path, image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_detections\u001b[39m(model_path, image):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 이미지 전처리\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     preprocessed_image \u001b[38;5;241m=\u001b[39m preprocess_image(image)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/engine/model.py:94\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/engine/model.py:145\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpt_path\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m weights, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m task \u001b[38;5;129;01mor\u001b[39;00m guess_model_task(weights)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ultralytics/utils/checks.py:374\u001b[0m, in \u001b[0;36mcheck_file\u001b[0;34m(file, suffix, download, hard)\u001b[0m\n\u001b[1;32m    372\u001b[0m files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;28mstr\u001b[39m(ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m file), recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# find file\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple files match \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, specify exact path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: '<ultralytics.models.yolo.model.YOLO object at 0x7fc8f0fae970>' does not exist"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "def merge_detections(detections_model1, detections_model2, iou_threshold=0.5):\n",
    "    merged_detections = detections_model1 + detections_model2\n",
    "    \n",
    "    final_detections = apply_nms(merged_detections, iou_threshold)\n",
    "    return final_detections\n",
    "\n",
    "detections_model1 = extract_detections(YOLO(\"/tf/notebook/ElementDetect.pt\"), image='/tf/notebook/1231.jpg')\n",
    "detections_model2 = extract_detections(YOLO(\"/tf/notebook/ElementDetect_swiper.pt\"), image='/tf/notebook/1231.jpg')\n",
    "\n",
    "\n",
    "final_detections = merge_detections(detections_model1, detections_model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaacc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detections(model_path, image):\n",
    "    # 모델 로드\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "\n",
    "    # 탐지 실행\n",
    "    detections = model(preprocessed_image)\n",
    "\n",
    "    # 결과 추출\n",
    "    extracted_detections = []\n",
    "    for detection in detections:\n",
    "        # 여기서는 간단하게 bounding box, 클래스 ID, 신뢰도를 추출\n",
    "        bbox = detection['bbox']\n",
    "        class_id = detection['class_id']\n",
    "        confidence = detection['confidence']\n",
    "        extracted_detections.append((bbox, class_id, confidence))\n",
    "\n",
    "    return extracted_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5385abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary: 295 layers, 25863847 parameters, 0 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(295, 25863847, 0, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = YOLO('/tf/notebook/runs/train17/weights/0912_step_1.pt')\n",
    "model1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae9917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.149 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24575MiB)\n",
      "Model summary (fused): 218 layers, 25847287 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /tf/notebook/step1/val/labels.cache... 2447 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2447/2447 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 153/153 [00:22<00:00,  6.68it/s]\n",
      "                   all       2447       5712      0.829      0.803      0.857      0.786\n",
      "                banner       2447       1118      0.861      0.851      0.921       0.88\n",
      "                2block       2447        411      0.749      0.742      0.805      0.752\n",
      "                3block       2447        797      0.874      0.872      0.915      0.863\n",
      "                4block       2447        908       0.88        0.9      0.941      0.892\n",
      "                5block       2447        435      0.906      0.862      0.932      0.863\n",
      "                6block       2447        184       0.89      0.918      0.936      0.863\n",
      "                7block       2447         31      0.922      0.839      0.883      0.835\n",
      "                8block       2447          9      0.529      0.778      0.818      0.702\n",
      "                nblock       2447         23      0.745      0.761      0.731       0.58\n",
      "                  typo       2447        245      0.746      0.437      0.621      0.544\n",
      "               gallery       2447        308      0.788      0.605      0.723      0.684\n",
      "                header       2447        636      0.955      0.925      0.955      0.855\n",
      "                footer       2447        607      0.938      0.953      0.967       0.91\n",
      "Speed: 0.1ms preprocess, 2.4ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.88046,     0.75241,     0.86297,     0.89152,     0.86335,     0.86283,     0.83463,     0.70179,     0.57984,      0.5445,     0.68352,     0.85451,     0.91035])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model1.val()  # 데이터셋과 설정을 기억하니 인수는 필요 없습니다.\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0b2a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Arguments received: ['yolo', 'help']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n",
      "                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n",
      "Ultralytics YOLOv8.0.149 🚀 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24575MiB)\n",
      "Setup complete ✅ (12 CPUs, 27.4 GB RAM, 137.9/251.0 GB disk)\n",
      "8.0.149\n",
      "💡 Learn about settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n",
      "Printing '\u001b[1m\u001b[30m/root/.config/Ultralytics/settings.yaml\u001b[0m'\n",
      "\n",
      "settings_version: 0.0.4\n",
      "datasets_dir: /tf/notebook/datasets\n",
      "weights_dir: weights\n",
      "runs_dir: runs\n",
      "uuid: 2213d21f39b86dc3305a5ec56546d98092177131a4d6a11c370062e0bc5e2115\n",
      "sync: true\n",
      "api_key: ''\n",
      "clearml: true\n",
      "comet: true\n",
      "dvc: true\n",
      "hub: true\n",
      "mlflow: true\n",
      "neptune: true\n",
      "raytune: true\n",
      "tensorboard: true\n",
      "wandb: true\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/ultralytics/cfg/default.yaml copied to /tf/notebook/default_copy.yaml\n",
      "Example YOLO command with this new custom cfg:\n",
      "    yolo cfg='/tf/notebook/default_copy.yaml' imgsz=320 batch=8\n",
      "Printing '\u001b[1m\u001b[30m/usr/local/lib/python3.8/dist-packages/ultralytics/cfg/default.yaml\u001b[0m'\n",
      "\n",
      "task: detect\n",
      "mode: train\n",
      "model: null\n",
      "data: null\n",
      "epochs: 100\n",
      "patience: 50\n",
      "batch: 16\n",
      "imgsz: 640\n",
      "save: true\n",
      "save_period: -1\n",
      "cache: false\n",
      "device: null\n",
      "workers: 8\n",
      "project: null\n",
      "name: null\n",
      "exist_ok: false\n",
      "pretrained: true\n",
      "optimizer: auto\n",
      "verbose: true\n",
      "seed: 0\n",
      "deterministic: true\n",
      "single_cls: false\n",
      "rect: false\n",
      "cos_lr: false\n",
      "close_mosaic: 10\n",
      "resume: false\n",
      "amp: true\n",
      "fraction: 1.0\n",
      "profile: false\n",
      "overlap_mask: true\n",
      "mask_ratio: 4\n",
      "dropout: 0.0\n",
      "val: true\n",
      "split: val\n",
      "save_json: false\n",
      "save_hybrid: false\n",
      "conf: null\n",
      "iou: 0.7\n",
      "max_det: 300\n",
      "half: false\n",
      "dnn: false\n",
      "plots: true\n",
      "source: null\n",
      "show: false\n",
      "save_txt: false\n",
      "save_conf: false\n",
      "save_crop: false\n",
      "show_labels: true\n",
      "show_conf: true\n",
      "vid_stride: 1\n",
      "line_width: null\n",
      "visualize: false\n",
      "augment: false\n",
      "agnostic_nms: false\n",
      "classes: null\n",
      "retina_masks: false\n",
      "boxes: true\n",
      "format: torchscript\n",
      "keras: false\n",
      "optimize: false\n",
      "int8: false\n",
      "dynamic: false\n",
      "simplify: false\n",
      "opset: null\n",
      "workspace: 4\n",
      "nms: false\n",
      "lr0: 0.01\n",
      "lrf: 0.01\n",
      "momentum: 0.937\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 3.0\n",
      "warmup_momentum: 0.8\n",
      "warmup_bias_lr: 0.1\n",
      "box: 7.5\n",
      "cls: 0.5\n",
      "dfl: 1.5\n",
      "pose: 12.0\n",
      "kobj: 1.0\n",
      "label_smoothing: 0.0\n",
      "nbs: 64\n",
      "hsv_h: 0.015\n",
      "hsv_s: 0.7\n",
      "hsv_v: 0.4\n",
      "degrees: 0.0\n",
      "translate: 0.1\n",
      "scale: 0.5\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.5\n",
      "mosaic: 1.0\n",
      "mixup: 0.0\n",
      "copy_paste: 0.0\n",
      "cfg: null\n",
      "tracker: botsort.yaml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!yolo help\n",
    "!yolo checks\n",
    "!yolo version\n",
    "!yolo settings\n",
    "!yolo copy-cfg\n",
    "!yolo cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f21a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
