{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Preprocessing"
      ],
      "metadata": {
        "id": "D8GyV0NWLJnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qUv-1UqfBOf2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Tokenization\n",
        "\n",
        "> ## 1) 영어: NLTK(Natural Language ToolKit)\n",
        "\n",
        "- 전처리에 필요한 데이터를 다운로드하여 사용\n",
        "  - nltk.download('punkt') -> 마침표(.), 개행문자(\\n) 등의 정보 다운로드"
      ],
      "metadata": {
        "id": "UA48ZX-0LQam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQyIIeSFLOKA",
        "outputId": "abf33ce6-709c-4343-9416-f28c5ca46b8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (1) 문장 토큰화: sent_tokenize( )\n",
        "\n",
        "- 'sentences' String 정의"
      ],
      "metadata": {
        "id": "3_u_lY3QLW-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = 'The X-Files is an American science fiction drama television series \\\n",
        "created by Chris Carter. \\\n",
        "The original television series aired from September 10, 1993 \\\n",
        "to May 19, 2002 on Fox. \\\n",
        "The program spanned nine seasons, with 202 episodes.'"
      ],
      "metadata": {
        "id": "JixPpfT-LWD0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8ocLIF7vLgLH",
        "outputId": "00c2c765-690c-4e2f-c602-101e355cc7f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The X-Files is an American science fiction drama television series created by Chris Carter. The original television series aired from September 10, 1993 to May 19, 2002 on Fox. The program spanned nine seasons, with 202 episodes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "Cpk3kKvOLiOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "sent_tokenize(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjZznX97LhH5",
        "outputId": "80807497-af66-40d9-e14c-677205a50f22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The X-Files is an American science fiction drama television series created by Chris Carter.',\n",
              " 'The original television series aired from September 10, 1993 to May 19, 2002 on Fox.',\n",
              " 'The program spanned nine seasons, with 202 episodes.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (2) 단어 토큰화: word_tokenize( )\n",
        "\n",
        "- 'text' String 정의\n"
      ],
      "metadata": {
        "id": "M3aJh_oYLqe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The truth is out there'"
      ],
      "metadata": {
        "id": "yu_T08vmLmZv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cU8h0ZQ7LuI3",
        "outputId": "d9470ad8-56cd-4249-95d1-6287055b50c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The truth is out there'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "MO6wa_9pLvQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAeIn73MLuXt",
        "outputId": "9f59e227-a75b-4095-c519-7121c234c543"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'truth', 'is', 'out', 'there']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (3) 단어 품사(Part Of Speech) 태깅: pos_tag( )\n",
        "\n",
        "- 품사 태깅 정보 다운로드"
      ],
      "metadata": {
        "id": "4ImYMENoL3TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0UiDS4sL08z",
        "outputId": "901d08ae-d0c1-414c-a35a-b86b5d10f956"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "2zQnv8pWL-rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "\n",
        "x = word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uERVa9A6L9yS",
        "outputId": "e722a2f9-4da7-4411-fcb0-a44ab1ea9737"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'), ('truth', 'NN'), ('is', 'VBZ'), ('out', 'RP'), ('there', 'RB')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS\n",
        "CC coordinating conjunction (등위 접속사)\n",
        "\n",
        "CD cardinal digit (기수)\n",
        "\n",
        "DT determiner (한정사)\n",
        "\n",
        "POS tag list:\n",
        "\n",
        "EX existential there (존재 구문) (like: \"there is\" ... think of it like \"there exists\")\n",
        "\n",
        "FW foreign word (외래어)\n",
        "\n",
        "IN preposition/subordinating conjunction (접속사/종속 접속사)\n",
        "\n",
        "JJ adjective 'big' (형용사)\n",
        "\n",
        "JJR adjective, comparative 'bigger' (형용사, 비교급)\n",
        "\n",
        "JJS adjective, superlative 'biggest' (형용사, 최상급)\n",
        "\n",
        "LS list marker (목록 표시, 1), 2), i, ii 등)\n",
        "\n",
        "MD modal could, will (조동사)\n",
        "\n",
        "NN noun, singular 'desk' (명사 단수)\n",
        "\n",
        "NNS noun plural 'desks' (명사 복수)\n",
        "\n",
        "NNP proper noun, singular 'Harrison' (고유명사, 단수)\n",
        "\n",
        "NNPS proper noun, plural 'Americans' (고유명사, 복수)\n",
        "\n",
        "PDT predeterminer 'all the kids' (전치 한정사)\n",
        "\n",
        "POS possessive ending parent's (소유격 말어미)\n",
        "\n",
        "PRP personal pronoun I, he, she (인칭대명사)\n",
        "\n",
        "PRP$ possessive pronoun my, his, hers (소유격대명사)\n",
        "\n",
        "RB adverb very, silently (부사)\n",
        "\n",
        "RBR adverb, comparative better (부사, 비교급)\n",
        "\n",
        "RBS adverb, superlative best (부사, 최상급)\n",
        "\n",
        "RP particle give up (소사)\n",
        "\n",
        "TO to go 'to' the store. (전치사 to)\n",
        "\n",
        "UH interjection errrrrrrrm (감탄사)\n",
        "\n",
        "VB verb, base form take (동사)\n",
        "\n",
        "VBD verb, past tense took (동사, 과거시제)\n",
        "\n",
        "VBG verb, gerund/present participle taking (현재분사)\n",
        "\n",
        "VBN verb, past participle taken (과거분사)\n",
        "\n",
        "VBP verb, sing. present, non-3d take (동사, 단수, 현재, 비3인칭)\n",
        "\n",
        "VBZ verb, 3rd person sing. present takes (동사, 3인칭 단수, 현재)\n",
        "\n",
        "WDT wh-determiner which (관계한정사)\n",
        "\n",
        "WP wh-pronoun who, what (관계대명사)\n",
        "\n",
        "WP$ possessive wh-pronoun whose (소유격 관계대명사)\n",
        "\n",
        "WRB wh-abverb where, when (관계부사)\n",
        "\n"
      ],
      "metadata": {
        "id": "GDv19_OsMFkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (4) Stop Words\n",
        "\n",
        "- 문법의 특성으로 존재하지만 문맥적으로 의미가 없는 단어\n",
        "  - 'stopwords' 정보 다운로드"
      ],
      "metadata": {
        "id": "gZVL5dzOMPq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1sc9xlIMEdT",
        "outputId": "363c7253-4dae-433d-c430-959af9cde506"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 'English' Stop Words 정보 확인\n",
        "  - 개수 및 내용 확인"
      ],
      "metadata": {
        "id": "dPavdGG_MVYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('English stop words : ', len(nltk.corpus.stopwords.words('english')))\n",
        "\n",
        "print(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFqbLMiYMUJl",
        "outputId": "28ab9ccb-4647-4d21-889d-943c86802733"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English stop words :  179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenize_text( ) 정의\n",
        "  - 여러개의 문장별로 단어 토큰을 생성하는 함수 정의"
      ],
      "metadata": {
        "id": "oAXlPuOsMmHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "def tokenize_text(doc):\n",
        "    sentences = sent_tokenize(doc)\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens"
      ],
      "metadata": {
        "id": "-1P54Z-fMhVa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장별 단어 토큰화 수행"
      ],
      "metadata": {
        "id": "as2iUDQzM68u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = tokenize_text(sentences)"
      ],
      "metadata": {
        "id": "eXSM6m7OM5mQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장별 단어 토큰화 결과 확인"
      ],
      "metadata": {
        "id": "uIs5LLZbNOXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in word_tokens:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2vJ4uEKM_Kd",
        "outputId": "8aba538d-2d75-415d-d86f-d334e391cb64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'X-Files', 'is', 'an', 'American', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'by', 'Chris', 'Carter', '.']\n",
            "['The', 'original', 'television', 'series', 'aired', 'from', 'September', '10', ',', '1993', 'to', 'May', '19', ',', '2002', 'on', 'Fox', '.']\n",
            "['The', 'program', 'spanned', 'nine', 'seasons', ',', 'with', '202', 'episodes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stop Words 제거"
      ],
      "metadata": {
        "id": "N4jWhwBLNSet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)"
      ],
      "metadata": {
        "id": "Ob0Uqw5WNRU8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stop Words 처리 결과"
      ],
      "metadata": {
        "id": "PTo0edT4NqHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in all_tokens:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISqiUR8HNV7P",
        "outputId": "011387d8-2ac3-47b8-9e9f-2ac7871bdb37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.']\n",
            "['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.']\n",
            "['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (5) Stemming(어간 추출)\n",
        "\n",
        "- 변화된 단어의 원형화 처리\n",
        "  - work\n",
        "  - amuse\n",
        "  - happy\n",
        "  - fancy"
      ],
      "metadata": {
        "id": "Pwrcw978NuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "7tlx33M9NsRb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "AXtdfmPxN4eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QInt4vC8N2RO",
        "outputId": "c61372aa-0c3c-48b6-87fb-5b3fe7dbbd44"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (6) Lemmatization(표제어 추출)\n",
        "\n",
        "- 변화된 단어의 원형화 처리\n",
        "  - Stemming 보다 정확한 처리 가능\n",
        "  - '품사'를 지정하여 사용"
      ],
      "metadata": {
        "id": "qCfqBYpBOD1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P5LML8hOBKw",
        "outputId": "a74ff69d-40be-4001-d561-81d220204ae1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 'v' 동사, 'a' 형용사"
      ],
      "metadata": {
        "id": "4WQprhjoOMMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "2HVHEzLDOLYn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## 2) 한국어: KoNLPy\n",
        "\n",
        "> ## (1) KoNLPy 패키지 설치"
      ],
      "metadata": {
        "id": "ehc1GjSuOSsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSIDrc18OQk3",
        "outputId": "8f6784a8-9422-4b1d-b1d7-affdc3820f5f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (2) Okt 형태소 분석기(Open Korea Text, Twitter)\n",
        "\n",
        "- 형태소(Morpheme)"
      ],
      "metadata": {
        "id": "ncN9f9HOOXB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "NL4oEZ6wOVTy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화 : okt.morphs( )"
      ],
      "metadata": {
        "id": "JFtKhuaEOcO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "print(okt.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAD8Upx0Oa6r",
        "outputId": "5d0eb06f-ccdf-40be-fc2c-cc33789a8f51"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지난', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인해', '전례', '없는', '고통', '을', '겪으며', '다양한', '방식', '으로', '심각하게', '피해', '를', '겪었습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 품사 태깅 : okt.pos( )"
      ],
      "metadata": {
        "id": "d3uKNZOAOl-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1BUAHI2OiqH",
        "outputId": "673832d7-7c5b-4004-cfc7-3241984614d8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('지난', 'Noun'), ('몇', 'Noun'), ('달', 'Noun'), ('간', 'Suffix'), ('전', 'Noun'), ('세계', 'Noun'), ('모든', 'Noun'), ('사람', 'Noun'), ('은', 'Josa'), ('코로나', 'Noun'), ('19', 'Number'), ('로', 'Noun'), ('인해', 'Adjective'), ('전례', 'Noun'), ('없는', 'Adjective'), ('고통', 'Noun'), ('을', 'Josa'), ('겪으며', 'Verb'), ('다양한', 'Adjective'), ('방식', 'Noun'), ('으로', 'Josa'), ('심각하게', 'Adjective'), ('피해', 'Noun'), ('를', 'Josa'), ('겪었습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 명사 추출 : okt.nouns( )"
      ],
      "metadata": {
        "id": "AYynTVqPOrxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37An48-HOqeg",
        "outputId": "400706cd-3131-4907-e2ad-cba14fdf453f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지난', '몇', '달', '전', '세계', '모든', '사람', '코로나', '로', '전례', '고통', '방식', '피해']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (3) Kkma 형태소 분석기\n",
        "\n",
        "- 형태소(Morpheme)"
      ],
      "metadata": {
        "id": "mDnzKQv6Ov0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma"
      ],
      "metadata": {
        "id": "8kfUEbxYOvIn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화 : kkma.morphs( )"
      ],
      "metadata": {
        "id": "TcSBzfdsO0-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "\n",
        "print(kkma.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuImiPW2O0R2",
        "outputId": "ead5a51b-9fb1-4466-e26a-775fb6c40cec"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지나', 'ㄴ', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인하', '어', '전례', '없', '는', '고통', '을', '겪', '으며', '다양', '하', 'ㄴ', '방식', '으로', '심각', '하', '게', '피해', '를', '겪', '었', '습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 품사 태깅 : kkma.pos( )"
      ],
      "metadata": {
        "id": "XwOrpBDwO6SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(kkma.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpH2BKCRO4td",
        "outputId": "a1347f2f-a8c7-4aa4-e5d6-ef57feeb9460"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('지나', 'VV'), ('ㄴ', 'ETD'), ('몇', 'MDT'), ('달', 'NNG'), ('간', 'NNB'), ('전', 'NNG'), ('세계', 'NNG'), ('모든', 'MDT'), ('사람', 'NNG'), ('은', 'JX'), ('코로나', 'NNG'), ('19', 'NR'), ('로', 'JKM'), ('인하', 'VV'), ('어', 'ECS'), ('전례', 'NNG'), ('없', 'VA'), ('는', 'ETD'), ('고통', 'NNG'), ('을', 'JKO'), ('겪', 'VV'), ('으며', 'ECE'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('방식', 'NNG'), ('으로', 'JKM'), ('심각', 'XR'), ('하', 'XSA'), ('게', 'ECD'), ('피해', 'NNG'), ('를', 'JKO'), ('겪', 'VV'), ('었', 'EPT'), ('습니다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 명사 추출 : kkma.nouns( )"
      ],
      "metadata": {
        "id": "h5eELKWBO9al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(kkma.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKzd6oN-O8rP",
        "outputId": "0c02566f-677e-47bf-e0b5-3c832abf02f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['달', '달간', '간', '전', '세계', '사람', '코로나', '코로나19', '19', '전례', '고통', '다양', '방식', '피해']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Encoding(Vectorization)\n",
        "\n",
        ">## 1) Encoding with TensorFlow\n",
        "\n",
        "> ## (1) Import Package"
      ],
      "metadata": {
        "id": "525eahupPC42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "HlFbeEzLPB_U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ##  (2) 실습 문장"
      ],
      "metadata": {
        "id": "QJxI2vkHPNvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '가지마라 가지마라 그녀는 위험해 매력이 너무 넘치는 Girl \\\n",
        "하지마라 하지마라 사랑은 위험해 \\\n",
        "내가 내가 내가 먼저 네게 네게 네게 빠져 빠져 빠져 버려 baby'"
      ],
      "metadata": {
        "id": "eSGCKpDAPMXE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "KhbVxJdePSNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "s3MHcrIlPRiN",
        "outputId": "2a1d31bd-6cf2-45ce-a859-45bc3234809a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'가지마라 가지마라 그녀는 위험해 매력이 너무 넘치는 Girl 하지마라 하지마라 사랑은 위험해 내가 내가 내가 먼저 네게 네게 네게 빠져 빠져 빠져 버려 baby'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## 2) 정수 인코딩(Integer Encoding)\n",
        "\n",
        "> ## (1) Tokenizer.fit_on_texts( )\n",
        "\n",
        "- Tokenization & Integer Indexing"
      ],
      "metadata": {
        "id": "t8icFSUsPWDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tknz = Tokenizer()\n",
        "tknz.fit_on_texts([sentence])"
      ],
      "metadata": {
        "id": "2W5OsgZcPTtS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "cbtEY3TDPjKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tknz.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH5Ljqm_Ph6N",
        "outputId": "c0345806-0880-45d9-c49c-173a638b8680"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'내가': 1,\n",
              " '네게': 2,\n",
              " '빠져': 3,\n",
              " '가지마라': 4,\n",
              " '위험해': 5,\n",
              " '하지마라': 6,\n",
              " '그녀는': 7,\n",
              " '매력이': 8,\n",
              " '너무': 9,\n",
              " '넘치는': 10,\n",
              " 'girl': 11,\n",
              " '사랑은': 12,\n",
              " '먼저': 13,\n",
              " '버려': 14,\n",
              " 'baby': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## (2) Tokenizer.texts_to_sequences( )\n",
        "\n",
        "- Integer Encoding"
      ],
      "metadata": {
        "id": "2JimdzAnPmNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LBE = tknz.texts_to_sequences([sentence])"
      ],
      "metadata": {
        "id": "julhLUdIPk5J"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "Ab6O37IfPrsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(LBE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-OWhz4hPqxb",
        "outputId": "94e913d8-7e93-4e6e-8939-5e749733f4c1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 4, 7, 5, 8, 9, 10, 11, 6, 6, 12, 5, 1, 1, 1, 13, 2, 2, 2, 3, 3, 3, 14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## 3) 원-핫 인코딩(One-Hot Encoding)\n",
        "\n",
        "> ## (1) to_categorical( )\n",
        "\n",
        "- One-Hot Encoding"
      ],
      "metadata": {
        "id": "BFTQyLsLPvLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "OHE = to_categorical(LBE)"
      ],
      "metadata": {
        "id": "YfPqcuY9PsrT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과 확인"
      ],
      "metadata": {
        "id": "w_bDB_jVP331"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(OHE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U-KlcFwP26b",
        "outputId": "aeab8309-04f2-444b-a03a-8616c8e7a156"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QW7uNmMjP5Wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}